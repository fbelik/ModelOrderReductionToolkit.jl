var documenterSearchIndex = {"docs":
[{"location":"index.html#ModelOrderReductionToolkit.jl-Models-and-Reductors","page":"Models and Reductors","title":"ModelOrderReductionToolkit.jl Models and Reductors","text":"","category":"section"},{"location":"index.html#Stationary-Models","page":"Models and Reductors","title":"Stationary Models","text":"A model <: StationaryModel{NOUT} is a map from parameter space to a vector space. When calling model(p, i=1) for i=1,...,NOUT, must return a vector of length output_length(model) and of eltype output_type(model). ","category":"section"},{"location":"index.html#Linear-Model","page":"Models and Reductors","title":"Linear Model","text":"A model of the form\n\nA(p) x(p) = b(p)\n\nFor an example, see PoissonModel().","category":"section"},{"location":"index.html#Linear-Matrix-Model","page":"Models and Reductors","title":"Linear Matrix Model","text":"A model of the form\n\nA(p) X(p) = B(p)\n\nFor an example, see to_frequency_domain(PenzlModel()).","category":"section"},{"location":"index.html#Stationary-Model-Reductors","page":"Models and Reductors","title":"Stationary Model Reductors","text":"Reduced basis methods for stationary models typically require the models to have implemented galerkin_project(model, V[, W=V], r=-1) and galerkin_add!(rom, fom, v, Vold[, w=v, Wold=Vold]).","category":"section"},{"location":"index.html#POD-Reductor","page":"Models and Reductors","title":"POD Reductor","text":"Proper orthogonal decomposition which solves the full order model at each parameter value, computes an SVD of the snapshot matrix, and allows for projection onto the resulting left singular vectors.\n\nmodel = PoissonModel()\nparams = [[i,j,k] for i in range(0,1,5) for j in range(0,1,5) for k in range(0,1,5)]\nreductor = PODReductor(model)\nadd_to_rb!(reductor, params) # or add_to_rb!(reductor, snapshots) if snapshot matrix already formed\nrom = form_rom(reductor, 10)","category":"section"},{"location":"index.html#SG-Reductor","page":"Models and Reductors","title":"SG Reductor","text":"Strong greedy method which solves the full order model at each parameter value, computes a column-pivoted QR decomposition of the snapshot matrix, and allows for projection onto the resulting orthonormalized snapshots.\n\nmodel = PoissonModel()\nparams = [[i,j,k] for i in range(0,1,5) for j in range(0,1,5) for k in range(0,1,5)]\nreductor = SGReductor(model)\nadd_to_rb!(reductor, params) # or add_to_rb!(reductor, snapshots) if snapshot matrix already formed\nrom = form_rom(reductor, 10)","category":"section"},{"location":"index.html#WG-Reductor","page":"Models and Reductors","title":"WG Reductor","text":"Weak greedy method which utilizes an ErrorEstimator to attempt to choose the parameter value (and index) for which the error is the worst for the given RB. Then, forms that full order solution and continues.\n\nmodel = PoissonModel()\nparams = [[i,j,k] for i in range(0,1,5) for j in range(0,1,5) for k in range(0,1,5)]\nscm = SCM(model.Ap, params, coercive=true)\nestimator = StabilityResidualErrorEstimator(model, scm)\nreductor = WGReductor(model, estimator)\nadd_to_rb!(reductor, params, 10)\nrom = form_rom(reductor, 10)\n\nSee sources 1-3 on more information on successive constraint methods, residual computations, and methodology on weak greedy reduced basis methods.","category":"section"},{"location":"index.html#Nonstationary-Models","page":"Models and Reductors","title":"Nonstationary Models","text":"A model <: NonstationaryModel is a map from parameter space to a set of ODEs. Calling model(p) initializes the model to the given parameter value. ODE outputs are vector of length output_length(model) and of eltype output_type(model). Can convert into an ODEProblem by to_ode_problem(model[, x0, tspan, p]).","category":"section"},{"location":"index.html#LTI-Model","page":"Models and Reductors","title":"LTI Model","text":"A model of the form\n\nE(p) x(tp) = A(p) x(tp) + B(p) u(t)\ny(tp) = C(p) x(tp) + D(p) u(t)\n\nAlso implements to_ss(model[, p=nothing]) to convert a model to a ControlSystems.jl state space, and to_dss(model[, p=nothing]) to convert to a DescriptorSystems.jl. Also implements to_frequency_domain(model) which returns a LinearMatrixModel to solve for the state variable x in the frequency domain. Also has galerkin_project(model, V[, W=V; r=-1]) implemented for forming reduced order models. To cast to an ODE problem for a given input u(x), call to_ode_problem(model[, x0=0.0, tspan=(0,1), p=nothing, u]).","category":"section"},{"location":"index.html#BT-Reductor","page":"Models and Reductors","title":"BT Reductor","text":"The state of the art method for reducing a non-parameterized LTI problem is through balanced truncation. This can be performed with a BTReductor object.\n\nAfter forming a BTReductor object on an LTIModel, can obtain the system Gramians through reachability_gramian(reductor) and observability_gramian(reductor). As with other reductors, has form_rom and lift implemented.\n\nmodel = PenzlModel()\nreductor = BTReductor(model)\nrom = form_rom(reductor, 20)\n\nSee source 4 for more information on the iterative method for solving Lyapunov equations used by BTReductor when iterative==true, and see MatrixEquations.jl for the non-sparse Lyapunov solver. See source 5 for the Penzl model example and for more information on truncation of LTI systems.","category":"section"},{"location":"index.html#RB-Reduction","page":"Models and Reductors","title":"RB Reduction","text":"For reducing a parameterized LTI problem in a reduced basis setting, one option is to create a (complex) basis in the frequency domain.\n\nmodel = ParameterizedPenzlModel()\nfreq_model = to_frequency_domain(model)\nreductor = PODReductor(freq_model)\nparams = [[ω,p1,p2,p3] for ω in range(-500,500,21) for p1 in range(-25,25,6) for p2 in range(-25,25,6) for p3 in range(-25,25,6)]\nreductor = PODReductor(freq_model)\nadd_to_rb!(reductor, params)\nrom = galerkin_project(model, Matrix(reductor.V[:,1:20])) # Faster when converting from VOV to Matrix","category":"section"},{"location":"index.html#References:","page":"Models and Reductors","title":"References:","text":"D.B.P. Huynh, G. Rozza, S. Sen, A.T. Patera. A successive constraint linear optimization method for lower bounds of parametric coercivity and inf–sup stability constants. Comptes Rendus Mathematique. Volume 345, Issue 8. 2007. Pages 473-478. https://doi.org/10.1016/j.crma.2007.09.019.\nQuarteroni, Alfio, Andrea Manzoni, and Federico Negri. Reduced Basis Methods for Partial Differential Equations. Vol. 92. UNITEXT. Cham: Springer International Publishing, 2016. http://link.springer.com/10.1007/978-3-319-15431-2.\nYanlai Chen, Jiang Jiahua, and Akil Narayan. A robust error estimator and a residual-free error indicator for reduced basis methods. Computers & Mathematics with Applications. 2019. http://www.sciencedirect.com/science/article/pii/S0898122118306850\nPatrick Kürschner and Peter Benner. Efficient low-rank solutions of large-scale matrix equations. Forschungsberichte aus dem Max-Planck-Institut für Dynamik Komplexer Technischer Systeme. 2016. https://pure.mpg.de/rest/items/item22467967/component/file_2296741/content.\nThilo Penzl. Algorithms for model reduction of large dynamical systems. Linear Algebra and its Applications. Volume 415, Issue 2, Pages 322-343. June 1, 2006. https://www.sciencedirect.com/science/article/pii/S0024379506000371.\nD.B.P. Huynh, D.J. Knezevic, Y. Chen, J.S. Hesthaven, A.T. Patera. A natural-norm Successive Constraint Method for inf-sup lower bounds. Computer Methods in Applied Mechanics and Engineering. Volume 199, Issue 29. June 1, 2010. https://www.sciencedirect.com/science/article/pii/S0045782510000691.","category":"section"},{"location":"index.html#ModelOrderReductionToolkit.LinearModel","page":"Models and Reductors","title":"ModelOrderReductionToolkit.LinearModel","text":"model = LinearModel(Ap::APArray, bp::APArray) <: StationaryModel{1}\nmodel = LinearModel(Ap::APArray, b::AbstractVector) <: StationaryModel{1}\n\nStruct for containing a parameterized linear model A(p) x = b(p) or A(p) x = b with affine parameter  dependence. Can form a solution for a new parameter value by  calling it on a new parameter value x = model(p).\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.LinearMatrixModel","page":"Models and Reductors","title":"ModelOrderReductionToolkit.LinearMatrixModel","text":"LinearMatrixModel{NOUT}\n\nmodel = LinearMatrixModel(Ap::APArray, bps::AbstractVector{Union{APArray, <:AbstractVector}})\nmodel = LinearMatrixModel(Ap::APArray, Bp::APArray)\nmodel = LinearMatrixModel(Ap::APArray, B::AbstractMatrix)\n\nStruct for containing a parameterized linear model A(p) X = B(p), with affine parameter dependence. Stored internally similarly to a LinearModel, so can solve for a single column of the solution by model(p, i) for i=1,...,NOUT, or can solve for the matrix solution by model(p, 0).\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.PODReductor","page":"Models and Reductors","title":"ModelOrderReductionToolkit.PODReductor","text":"pod_reductor <: PODReductor\n\nA struct for holding the parts of a POD reductor for a StationaryModel model. Can access the FOM through  pod_reductor.model, the snapshot matrix through  pod_reductor.snapshots, the singular values through  pod_reductor.S, and the reduced basis  (left singular vectors) through pod_reductor.V.\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.SGReductor","page":"Models and Reductors","title":"ModelOrderReductionToolkit.SGReductor","text":"sg_reductor <: SGReductor\n\nA struct for holding the parts of an SG reductor for  a StationaryModel model. Can access the FOM through  sg_reductor.model, the snapshot matrix through  sg_reductor.snapshots, the pivot order through  sg_reductor.p, and the reduced basis through  sg_reductor.V.\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.WGReductor","page":"Models and Reductors","title":"ModelOrderReductionToolkit.WGReductor","text":"wg_reductor <: WGReductor\n\nStores a FOM in model, an error estimator estimator,  the greedily selected parameters params_greedy, the reduced basis in V, the ROM in rom, the approximate errors at each step in approx_errors, and the truth errors in each step at truth_errors.\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.LTIModel","page":"Models and Reductors","title":"ModelOrderReductionToolkit.LTIModel","text":"model = LTIModel(A_in, B_in, C_in, D_in=0, E_in=I) <: NonstationaryModel model = LTIModel(lti<:AbstractStateSpace) <: NonstationaryModel model = LTIModel(lti<:AbstractDescriptorStateSpace) <: NonstationaryModel\n\nStruct for containing a parameterized LTI model E(p) x'(t,p) = A(p) x(t,p) + B(p) u(t) y(t,p) = C(p) x(t,p) + D(p) u(t) with affine parameter dependence where u(t) is some input signal. Can initialize the system to a given  parameter p by calling model(p).\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.BTReductor","page":"Models and Reductors","title":"ModelOrderReductionToolkit.BTReductor","text":"reductor = BTReductor(model::LTIModel[, p=nothing; noise=0, iterative=nothing, maxdim=-1, lradi_eps=1e-6, dense_row_tol=1e-8])\n\nBalanced truncation reductor object for reducing an LTIModel. If parameter p passed in, model first initialized to parameter value. noise determines amount of printed output. If isnothing(iterative), checks the size and sparsity of the system whether or not to use an iterative method. If iterative==true,  uses an iterative low-rank ADI method to solve for the Gramians (see Kurschner and Benner 2016 Dissertation Alg 4.3), otherwise, uses MatrixEquations.jl to solve them densely. maxdim determines the  maximum rank for the iterative solver, and lradi_eps determines a tolerance for when the algorithm terminantes. If iterative==false, dense_row_tol is used to  truncate the dense Gramians for quicker solving of the RB and HSVs.\n\nThe reachability Gramian can be computed by reductor.R * reductor.R', and the  observability Gramian by reductor.L * reductor.L'.\n\nHSVs stored in reductor.hs.\n\nPetrov-Galerkin test and trial spaces stored in reductor.V and reductor.W respectively.\n\nInitialized-to parameter value stored in reductor.p.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#Additional-ModelOrderReductionToolkit.jl-Docstrings","page":"Additional Docstrings","title":"Additional ModelOrderReductionToolkit.jl Docstrings","text":"","category":"section"},{"location":"docs.html#Models","page":"Additional Docstrings","title":"Models","text":"","category":"section"},{"location":"docs.html#Reductors","page":"Additional Docstrings","title":"Reductors","text":"","category":"section"},{"location":"docs.html#Affinely-parameter-dependent-arrays:","page":"Additional Docstrings","title":"Affinely parameter-dependent arrays:","text":"","category":"section"},{"location":"docs.html#Matrices-as-vector-of-vectors:","page":"Additional Docstrings","title":"Matrices as vector of vectors:","text":"","category":"section"},{"location":"docs.html#Successive-constraint-method-(SCM):","page":"Additional Docstrings","title":"Successive constraint method (SCM):","text":"","category":"section"},{"location":"docs.html#Radial-basis-interpolatory-stability-factor","page":"Additional Docstrings","title":"Radial-basis interpolatory stability factor","text":"","category":"section"},{"location":"docs.html#Computation-of-norm-of-residual","page":"Additional Docstrings","title":"Computation of norm of residual","text":"","category":"section"},{"location":"docs.html#Linear-algebra-utilities","page":"Additional Docstrings","title":"Linear algebra utilities","text":"","category":"section"},{"location":"docs.html#ModelOrderReductionToolkit.galerkin_project","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.galerkin_project","text":"rom = galerkin_project(model::LinearModel, V[, W=V; r=-1])\n\nPerform (Petrov) Galerkin projection on a linear model where the trial space is the first r columns of V and the test space is the first r columns of W. If r=-1, uses all columns. Returns a new LinearModel.\n\nW' * A(p) * V * x_r = W' * b(p),V * x_r ≈ x = A(p)^(-1) b(p)`\n\n\n\n\n\nrom = galerkin_project(model::LinearMatrixModel, V[, W=V; r=-1])\n\nPerform (Petrov) Galerkin projection on each linear model in model.models.\n\n\n\n\n\ngalerkin_project(model, V[, W=V; WTEVisI=false, r=-1])\n\nPerforms Galerkin projection on the model <: LTIModel and returns a new LTIModel. By default, assumes that WᵀV=I, if WTEVisI==true, then assumes that WᵀEV=I.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.galerkin_add!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.galerkin_add!","text":"galerkin_add!(rom::LinearModel, fom::LinearModel, v, Vold[, w=v, Wold=Vold; r=-1])\n\nAssuming that rom = galerkin_project(model, Vold, Wold), updates rom such that if V = [Vold v] and W = [Wold w], then rom = galerkin_project(model, V, W).\n\n\n\n\n\ngalerkin_add!(rom::LinearMatrixModel, fom::LinearMatrixModel, v, Vold[, w=v, Wold=Vold; r=-1])\n\nAssuming that rom = galerkin_project(fom, Vold, Wold), updates rom such that if V = [Vold v] and W = [Wold w], then rom = galerkin_project(fom, V, W).\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.PoissonModel","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.PoissonModel","text":"model = PoissonModel([; Nx=999, P=3])\n\nUses finite differences on the following PDE to generate a LinearModel with parameter  dependence. Change discretization with Nx,  and change number of parameters, and hence number of affine terms, with P.\n\n- ∂_x (κ(x, p) ∂_x u(x, p)) = f(x)\n\nu(0,p) = 0; u(1,p) = p[1]\n\nκ(x,p) = (1.05 - (1/2)^(P-1)) + 0.5 p[2] sin(2πx) + 0.25 p[3] sin(4πx) + ... + (1/2)^(P-1) p[P] sin(2π * P * x)\n\nf(x,p) = (0.25 .< x .< 0.75) .* 10.0\n\nlength(p) = P; 0 ≤ p[i] ≤ 1\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.PenzlModel","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.PenzlModel","text":"model = PenzlModel()\n\nGenerates the standard Penzl LTIModel with one input, one output, and ns=1006 dimension state variable.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.MISOPenzlModel","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.MISOPenzlModel","text":"model = MISOPenzlModel()\n\nGenerates an LTIModel with three inputs, one output,  and a state of dimension ns=1006. Same structure as the  Penzl model except the B matrix is changed to\n\n1006×3 Matrix{Float64}:\n 10.0   0.0   0.0\n 10.0   0.0   0.0\n  0.0  10.0   0.0\n  0.0  10.0   0.0\n  0.0   0.0  10.0\n  0.0   0.0  10.0\n  1.0   1.0   1.0\n  ⋮\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n  1.0   1.0   1.0\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.to_frequency_domain","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.to_frequency_domain","text":"frequency_model = to_frequency_domain(model, logfreq=false)\n\nAssuming null initial conditions, uses the Laplace transform to convert the LTIModel into a LinearMatrixModel for which the first element of the parameter vector is the imaginary part of the frequency variable. The model is of the form sE(p) X = A(p) X + B(p) where X is the Laplace variable for X and s = 0 + iω. If logfreq=true, then scales ω such that p[1] = log10(ω).\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.to_ss","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.to_ss","text":"to_ss(model[, p=nothing])\n\nInitializes the model to the parameter p if passed in, then returns a ControlSystems.jl StateSpace object. \n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.to_dss","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.to_dss","text":"to_dss(model[, p=nothing])\n\nInitializes the model to the parameter p if passed in, then returns a DescriptorSystems.jl DescriptorStateSpace object. \n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.to_ode_problem","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.to_ode_problem","text":"to_ode_problem(model[, p=nothing; u=(t->zeros(size(model.B, 2))), x0=0.0, tspan=(0,1)])\n\nCreates an ODEProblem for the model <: LTISystem for a given input u(t). Note that this is the ODE for the state variable x. Once have formed the solution object, will have to multiply by model.C to get the output y. Note that DifferentialEquations.jl names the output u, which for this problem is the state variable x, not the input u.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.bode","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.bode","text":"bode(model::LTIModel, ω::Real[, p=nothing; first=true])\n\nReturns the transfer function evaluated at s=im*ω, C * (sE - A)^(-1) B + D, evaluated at the [1,1] entry if first==true.\n\n\n\n\n\nbode(model::LTIModel, ωs::AbstractVector{<:Union{AbstractVector,Real}}[, p=nothing; first=true])\n\nReturns the transfer function evaluated at s=im*ω for ω in ωs, evaluated at the [1,1] entry if first==true.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.ParameterizedPenzlModel","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.ParameterizedPenzlModel","text":"model = ParameterizedPenzlModel()\n\nGenerates an LTIModel with one input, one output,  and a state of dimension ns=1006. Same structure as the  Penzl model, expect depends on a parameter vector of length 3 which shift the poles along the complex axis. Instantiate to a parameter vector by calling model([p1,p2,p3]).\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.StabilityResidualErrorEstimator","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.StabilityResidualErrorEstimator","text":"StabilityResidualErrorEstimator <: ErrorEstimator\n\nError estimator for a LinearModel which approximates the  stability factor with stability_estimator(p) and computes  the residual norm with residual_norm.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.form_rom","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.form_rom","text":"form_rom(pod_reductor, r=-1)\n\nPulls the first r left singular vectors from  pod_reductor.decomp, and then Galerkin projects pod_reductor.model onto that basis, and returns the resulting ROM.\n\n\n\n\n\nform_rom(sg_reductor, r=-1)\n\nCalls galerkin_project on the FOM and returns a ROM with RB of dimension r. If r=-1, uses all available columns of sg_reductor.V.\n\n\n\n\n\nform_rom(wg_reductor, r=-1)\n\nCalls galerkin_project on the FOM and returns a ROM with RB of dimension r. If r=-1, uses all available columns of wg_reductor.V.\n\n\n\n\n\nform_rom(bt_reductor[, r=-1])\n\nUses Petrov-Galerkin on the model to form a ROM of order r (largest possible if r==-1). Also, initializes it to bt_reductor.p if not nothing.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.lift","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.lift","text":"lift(pod_reductor, x_r)\n\nGiven a solution array x_r to a ROM formed by the pod_reductor lifts the solution(s) to the same dimension of the FOM.  \n\n\n\n\n\nlift(sg_reductor, x_r)\n\nGiven a solution array x_r to a ROM formed by the sg_reductor lifts the solution(s) to the same dimension of the FOM. \n\n\n\n\n\nlift(wg_reductor, x_r)\n\nGiven a solution array x_r to a ROM formed by the wg_reductor lifts the solution(s) to the same dimension of the FOM. \n\n\n\n\n\nlift(bt_reductor, x_r)\n\nGiven a solution array x_r to a ROM formed by the bt_reductor lifts the solution(s) to the same dimension of the FOM.   \n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.add_to_rb!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.add_to_rb!","text":"add_to_rb!(pod_reductor, snapshots[; noise=1])\n\nDirectly updates pod_reductor with new snapshots given in the columns of the matrix snapshots.\n\n\n\n\n\nadd_to_rb!(pod_reductor, parameters[; noise=0, progress=false])\n\nLoops through the vector of parameters, forms their full order solutions, adds them to pod_reductor.snapshots, and then updates the singular values  and singular vectors in pod_reductor.S and pod_reductor.V.\n\n\n\n\n\nadd_to_rb!(sg_reductor, snapshots[; noise=0])\n\nDirectly updates sg_reductor with new snapshots given in the columns of the matrix snapshots.\n\n\n\n\n\nadd_to_rb!(sg_reductor, parameters[; noise=0, progress=false])\n\nLoops through the vector of parameters, forms their full order solutions, adds them to sg_reductor.snapshots, and then updates the reduced basis in sg_reductor.V.\n\n\n\n\n\nadd_to_rb!(wg_reductor, params[; noise=0, progress=false, eps=0.0, zero_tol=1e-15])\n\nLoops through the vector of parameters params, computes the approximate estimator for each, selects the one with the highest error, and updates wg_reductor with  the corresponding full order solution. Returns true if a vector is added to the RB, false otherwise.\n\n\n\n\n\nadd_to_rb!(wg_reductor, params, r[; noise=0, eps=0.0, zero_tol=1e-15])\n\nAdds to wg_reductor at least r times by calling  add_to_rb!(wg_reductor, params, noise=noise, eps=eps, zero_tol=zero_tol) several times. If all r are added, returns true, otherwise false.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.get_rom","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.get_rom","text":"get_rom(wg_reductor)\n\nHelper method for getting the ROM from the  wg_reductor object. Can otherwise obtain it through wg_reductor.rom.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.APArray","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.APArray","text":"AffineParametrizedArray(arrays, makeθi[, precompθ=false])\n\nStruct for containing an affine parametrized array A(p) of the form A(p) = ∑ makeθi(p,i) * arrays[i].\n\nArrays are stored in the vector arrays for quick recomputation for new parameter values. Each must be of the same size and dimension so that they can be  broadcast summed.\n\nIf precompθ set to false (default):\n\nThe function makeθi(p,i) takes in a parameter object first, and second an index i=1,...,length(arrays), and returns a scalar\n\nIf precompθ set to true:\n\nThe function makeθi(p) takes in a parameter object, and returns a vector of each of the affine terms.\n\nGiven aparr <: AffineParametrizedArray, and a new parameter value p,  can form the full array A(p) by calling aparr(p).\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.formArray!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.formArray!","text":"formArray!(aparr, arr, p)\n\nGiven an array arr with the same dimensions as the arrays in the APArray aparr, form A(p) and place its values in arr.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.eim","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.eim","text":"eim(arrFun, param_disc[, ϵ=1e-2; maxM=100, noise=1])\n\nMethod for constructing an APArray object from a non-affinely parameter dependent matrix arrFun(p) by empirical interpolation. \n\nparam_disc must be a matrix with columns as parameter vectors, or a vector with elements as parameters.\n\nLoops over the given parameter discretization until a maximum ∞-norm error  of ϵ is achieved over the entire discretization, or until the maximum number of parameter values are chosen, given by maxM.\n\nnoise dictates the amount of printed output. Set 0 for no output, 1 for some output, ≥2 for most.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.VOV","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.VOV","text":"VectorOfVectors{T} <: AbstractMatrix{T}\n\nType for defining matrices as vectors of vectors for  quick insertion of rows and columns. Stores the columns as vectors in vecs and its size in size.\n\nConstruct an empty VOV of dimensions (nrows × ncols) where  one of nrows or ncols must be zero:\n\nVectorOfVectors(nrows=0, ncols=0, T=Float64)\n\nConstruct a VOV from a Julia vector of vectors:\n\nVectorOfVectors(vecs::AbstractVector{<:AbstractVector{T}})\n\nConstruct a VOV from a matrix:\n\nVectorOfVectors(M::AbstractMatrix{T})\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.addRow!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.addRow!","text":"addRow!(vov::VectorOfVectors)\n\nAdd a row to the vector of vectors by appending zero(T) to each vector.\n\n\n\n\n\naddRow!(vov::VectorOfVectors, row::AbstractVector)\n\nAdd the vector row to the last column of vov.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.removeRow!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.removeRow!","text":"removeRow!(vov::VectorOfVectors)\n\nRemove a row from the vector of vectors.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.addCol!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.addCol!","text":"addCol!(vov::VectorOfVectors)\n\nAdd a column to the vector of vectors by appending zeros(T,nrow) to vov.vecs.\n\n\n\n\n\naddCol!(vov::VectorOfVectors, col::AbstractVector)\n\nAdd the vector row to the last column of vov.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.removeCol!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.removeCol!","text":"removeCol!(vov::VectorOfVectors)\n\nRemove a column from the vector of vectors.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.SCM","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.SCM","text":"SCM{P} <: AbstractSCM <: Function where P <: Int\n\nAn SCM object for a SPD APArray matrix Ap. For non-SPD  matrix Ap, can form Grammian matrix Ap'Ap and perform SCM on it.\n\nscm = SCM(Ap::APArray, ps::AbstractVector[, ϵ=0.8, Mα=20, Mp=0; coercive=false,  optimizer=HiGHS.Optimizer, lp_attrs=Dict(), make_monotonic=true, max_iter=500, noise=0])\n\nConstructs an SCM object for Ap about parameter values ps. If coercive=false, then constructs the Grammian matrix Ap(p)'Ap(p). If hermitianpart(Ap(p)) is positive definite for all p, then set coercive=true. Constrains the SCM object to a relative gap of ϵ. If  set ϵ=nothing, then does not perform constraining. Mα and Mp are the stability and  positivity parameters, optimizer is the optimizer used for solving LPs in JuMP. lp_attrs can contain additional attributes to pass into the JuMP model (optimizer dependent).  make_monotonic ensures that the lower-bound predictions increase monotonically. max_iter  is the maximum number of SCM constraining iterations. noise determines the amount of printed output.\n\nThe scm object may be used by calling it on a parameter vector\n\nscm(p[, Mα=20, Mp=0; which=:L, noise=0])\n\nWhich returns the following:\n\nwhich=:L - The lower-bound prediction at p\nwhich=:U - The upper-bound prediction at p\nwhich=:E - The relative gap 1 - LB/UB at p\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.ANLSCM","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.ANLSCM","text":"ANLSCM{P} <: AbstractSCM <: Function where P <: Int\n\nAn SCM object for an APArray matrix Ap which uses nonlinear constraining. \n\nscm = ANLSCM(Ap::APArray, ps::AbstractVector[, ϵ=0.8, Mα=20; optimizer=Ipopt.Optimizer,  lp_attrs=Dict(), nonlin_alpha=0.0, max_iter=500, noise=0, constrain_kwargs...])\n\nConstructs an ANLSCM object for Ap about parameter values ps. Constrains the ANLSCM  object to a relative gap of ϵ. If set ϵ=nothing, then does not perform constraining.  Mα is the stability parameter. optimizer is the optimizer used for solving NLPs in JuMP. lp_attrs can contain additional attributes to pass into the JuMP model (optimizer dependent).  nonlin_alpha is the initial value of nonlinear constraint coefficients (0 for lightest constraint, 1 for tightest and LB guarantee). max_iter is the maximum number of SCM constraining iterations. noise determines the amount of printed output. Additional kwargs passed to constrain!.\n\nThe scm object may be used by calling it on a parameter vector\n\nscm(p[, Mα=20; which=:L, noise=0])\n\nWhich returns the following:\n\nwhich=:L - The lower-bound prediction at p\nwhich=:U - The upper-bound prediction at p\nwhich=:E - The relative gap 1 - LB/UB at p\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.NNSCM","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.NNSCM","text":"NNSCM{P} <: AbstractSCM <: Function where P <: Int\n\nAn SCM object for an APArray matrix Ap which uses domain decomposition and works in a natural norm. See Huynh et al., 2010, A natural norm...\n\nscm = NNSCM(Ap::APArray, ps::AbstractVector[, ϵ=0.8, ϵ_β=0.8, ϕ=0.0, Mα=20; optimizer=HiGHS.Optimizer,  lp_attrs=Dict(), max_iter=500, max_inner_iter=500, noise=0, constrain_kwargs...])\n\nConstructs an NNSCM object for Ap about parameter values ps. Constrains the NNSCM  object to a relative gap of ϵ. If set ϵ=nothing, then does not perform constraining.  ϵ_β is the natural-norm relative gap ensured within subdomains. ϕ is the minimum natural-norm lower-bound prediction required to be considered within a domain. Mα is the stability parameter. optimizer is the optimizer used for solving NLPs in JuMP. lp_attrs can contain additional attributes to pass into the JuMP model (optimizer dependent).  max_iter is the maximum number of SCM constraining iterations and max_inner_iter is the maximum number of iterations within a particular domain decomposition.  noise determines the amount of printed output. Additional kwargs passed to constrain!.\n\nThe scm object may be used by calling it on a parameter vector\n\nscm(p[, Mα=20; pbar=nothing, which=:L, noise=0])\n\nWhich returns the following:\n\nwhich=:L - The lower-bound prediction at p\nwhich=:U - The upper-bound prediction at p\nwhich=:E - The relative gap 1 - LB/UB at p\n\nIf pbar is not nothing, then if pbar in keys(scm.UBs), solves the above restricted to the domain decomposition about pbar.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.copy_scm","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.copy_scm","text":"copy_scm(scm::SCM[; lp_attrs=Dict(), noise=0])\n\nMakes a copy of scm and provides lp_attrs to the new JuMP model.\n\n\n\n\n\ncopy_scm(scm::ANLSCM[; nonlin_alpha=0.0, lp_attrs=Dict(), noise=0])\n\nMakes a copy of scm with initial nonlinear constraint coefficients nonlin_alpha  and provides lp_attrs to the new JuMP model.\n\n\n\n\n\ncopy_scm(scm::NNSCM[; lp_attrs=Dict(), noise=0])\n\nMakes a copy of scm and provides lp_attrs to the new JuMP model.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.constrain!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.constrain!","text":"constrain!(scm::SCM, ps::AbstractVector, ϵ::Real, Mα::Int, Mp::Int[; make_monotonic=true, max_iter=500, noise=0, reigkwargs...])\n\nConstrains scm about parameters ps to relative gap ϵ. Mα and Mp are the stability and positivity constraints respectively. make_monotonic ensures LB predictions increase monotonically. max_iter is the maximum number of SCM  iterations ran. noise determines amount of printed output. See documentation of ModelOrderReductionToolkit.reig for arguments to reigkwargs.\n\n\n\n\n\nconstrain!(scm::ANLSCM, ps::AbstractVector, ϵ::Real, Mα::Int[; adaptive_nl_rate=1.1, adaptive_eps_tol=1e-6, max_iter=500, noise=0, reigkwargs...])\n\nConstrains scm about parameters ps to relative gap ϵ. Mα is the stability constraint.  adaptive_nl_rate and adaptive_eps_tol determine when to and rate at which to loosen nonlinear constraints.  Set adaptive_nl_rate=nothing for no adaptive updating. max_iter is the maximum number of SCM iterations ran. noise determines amount of printed output. See ModelOrderReductionToolkit.reig for arguments to reigkwargs.\n\n\n\n\n\nconstrain!(scm::NNSCM, ps::AbstractVector, pbar::AbstractVector[, ϵ_β=0.8, ϕ=0.0, Mα=20, ps_left=trues(length(ps)); max_iter=500, p_choice=3, noise=0, reigkwargs...])\n\nConstrains the pbar-domain decomposition of the scm about parameters ps. ϵ_β determines the natural-norm relative gap, ϕ determines the minimum LB prediction required for inclusion in domain, Mα is the stability parameter, ps_left is a vector of which parameters to consider, max_iter is maximum number of iterations.\n\np_choice can be 1, 2, or 3. \n\np_choice=1 - Only add parameter with highest natural norm relative gap\np_choice=2 - Only add parameter within domain (LB ≥ ϕ) with highest natural norm relative gap\np_choice=3 - Do both of above\n\nSee ModelOrderReductionToolkit.reig for arguments to reigkwargs.\n\n\n\n\n\nconstrain!(scm::NNSCM, ps::AbstractVector, ϵ::Real[, ϵ_β=0.8, ϕ=0.0, Mα=20; pruning=false, removals=false, eps_keep=0.2, p_choice=3, max_iter=500, max_inner_iter=500, noise=0, reigkwargs...])\n\nConstrains scm about parameters ps to relative gap ϵ with inner-maximum relative gap ϵ_β and domain-inclusion  parameter ϕ. Mα is the stability constraint. pruning determines whether or not to exclude parameters from consideration when their relative gap decreases below ϵ for a greedier procedure. removals determines whether or not to try removing unnecessary domains at the start of each iteration with tolerance eps_keep. See other constrain! method for p_choice. max_iter is the maximum number of SCM iterations ran and max_inner_iter determines the maximum number of iterations per domain decomposition. noise determines amount of printed output.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.Sigma_Min_RBF","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.Sigma_Min_RBF","text":"Sigma_Min_RBF\n\nA mutable struct used for approximating the minimum singular value of a matrix  with parametric dependence A(p). Given  a new parameter value p, approximate σ_min(A(p)) with sigma_min_rbf(p).\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.min_sigma_rbf","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.min_sigma_rbf","text":"min_sigma_rbf(params, Ais, makeθAi[, ϕ=gaussian_rbf])\n\nMethod to form a interpolatory radial-basis function functor to approximate the minimum singular value of a parametrized matrix A(p). Pass as input either a matrix params where each column is a parameter vector, or a vector  of vectors params where each vector is a parameter. \n\nOptional argument for the radial-basis function ϕ, which defaults to the Gaussian ϕ(r) = exp(-r^2).\n\nReturns a functor sigma_min_rbf <: Sigma_Min_RBF such that given a new parameter vector p, sigma_min_rbf(p) returns an approximation to the minimum singular value of A(p).\n\nOffline, solves for the minimum singular value for each parameter in params, and uses this to form an interpolatory approximation in the form of log(σ_min(A(p))) ≈ ω_0 + ∑ (ω_i p_i) + ∑ (γ_i ϕ(p - p_i)) where p_i is the i'th parameter in params, and ω_0, ω_i, and γ_i are determined by the given params such that the approximation holds true for all p_i, and that ∑ γ_i = 0, and that ∑ γ_i p_i[j] = 0 for each j.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.update_sigma_rbf!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.update_sigma_rbf!","text":"update_sigma_rbf!(sigma_min_rbf, ϕ)\n\nGiven a Sigma_Min_RBF object sigma_min_rbf, update its RBF, ϕ, and update the coefficients.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.ResidualNormComputer","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.ResidualNormComputer","text":"res <: ResidualNormComputer{T}\n\nAbstract type for computing the norm of the residual for weak greedy RB methods. Must implement the update!(res, v) and compute(res, u_r, p) methods.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.StandardResidualNormComputer","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.StandardResidualNormComputer","text":"StandardResidualNormComputer{T} <: ResidualNormComputer{T} StandardResidualNormComputer(Ap::APArray,bp::APArray,V=nothing,X=nothing)\n\nA struct for containing the necessary vectors and matrices for quickly compute the X-norm of the residual, r(u_r,p) = A(p) (u - V u_r) = b(p) - A(p) V u_r, by taking advantage of affine parameter dependence of A(p) and b(p).\n\nHere, u solves A(p) u = b(p) with A and b having affine parameter dependence, and V is a matrix with columns defining bases for approximation spaces u ≈ V u_r.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.ProjectionResidualNormComputer","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.ProjectionResidualNormComputer","text":"ProjectionResidualNormComputer{T} <: ResidualNormComputer{T} ProjectionResidualNormComputer(Ap::APArray,bp::APArray,V=nothing,X=nothing)\n\nA struct for containing the necessary vectors and matrices for quickly compute the X-norm of the residual, r(u_r,p) = A(p) (u - V u_r) = b(p) - A(p) V u_r, by taking advantage of affine parameter dependence of A(p) and b(p). Uses a projection method which is more stable than the standard method.\n\nHere, u solves A(p) u = b(p) with A and b having affine parameter dependence, and V is a matrix with columns defining bases for approximation spaces u ≈ V u_r.\n\n\n\n\n\n","category":"type"},{"location":"docs.html#ModelOrderReductionToolkit.full_lu","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.full_lu","text":"full_lu(A; steps=-1)\n\nPerforms a completely pivoted LU factorization on the matrix A, returning permutation vectors Q and P, and lower and upper triangular matrices L and U, such that if all steps are performed, then A[P,Q] = L*U.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.reig","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.reig","text":"reig(A::AbstractMatrix, [B=I; which=:L, kmaxiter=1000, noise=0, egwith=:arpack, restarts=100,  krylovsteps=8, eps=1e-14, randdisks=1000, ignoreB=false, minstep=1.0, force_sigma=nothing])\n\nGiven (symmetric) matrices A and B, computes a real eigenvalue\n\nA x = λ B x.\n\nwhich=:L corresponds to the largest eigenvalue\nwhich=:S corresponds to the smallest eigenvalue\nwhich=:SP corresponds to the smallest positive eigenvalue\n\nReturns a tuple (λ,v) where λ<:Real is the eigenvalue and v the eigenvector.\n\nAttempts to do this by shift-and-invert. Uses Arpack.jl if egwith=:arpack or ArnoldiMethod.jl if egwith=:arnoldimethod where the shifts are determined by the eigenvalue seeked and the Gershgorin disks of B⁻¹A.\n\nIf egwith=:arpack, kmaxiter determines maximum number of iterations. If  egwith=:arnoldimethod, uses restarts to determine number of restarts.\n\nNOTE: A randomized algorithm is used by default for determining Gershgorin disks. This is to speed up the case when size(A,1) ≫ 1.\n\nFollowing paramers relate to selection of shifts:\n\nkrylovsteps determines number of logarithmically spaced shifts are attempted\nignoreB determines whether to approximate Gershgorin disks of B⁻¹A or A\nranddisks determines number of (random) columns of B⁻¹A or A to subselect\n\nto compute Gershgorin disks of, others are ignored. Set randdisks ≥ size(A,1)  for no randomization.\n\nminstep determines the minimum step size between logarithmically spaced shifts\nforce_sigma forces trying shift and invert about this value if not nothing\n\nParameter eps is used to perturb shift parameters by relative amount to attempt to ensure no singular shifts. If the relative gap between the shift and the found eigenvalue is greater than reldifftol and the absolute gap is greater than absdifftol, attempts to re-solve by shifting about the found eigenvalue.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.smallest_sval","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.smallest_sval","text":"smallest_sval(A[; kmaxiter=1000, noise=0, reigkwargs...])\n\nGiven a matrix A, attempts to compute the smallest singular value of it by Krylov iteration and inversion. If unsuccessful, computes a full, dense svd. See docs for  ModelOrderReductionToolkit.reig for reigkwargs options. Returns the smallest singular value, σ_min<:Real\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.largest_sval","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.largest_sval","text":"largest_sval(A[; kmaxiter=1000, noise=0])\n\nGiven a matrix A, attempts to compute the largest singular value of it by Krylov iteration. If unsuccessful, computes a  full, dense svd. Returns the largest singular value,  σ_max<:Real.\n\n\n\n\n\n","category":"function"},{"location":"docs.html#ModelOrderReductionToolkit.orthonormalize_mgs2!","page":"Additional Docstrings","title":"ModelOrderReductionToolkit.orthonormalize_mgs2!","text":"orthonormalize_mgs2!(u, V)\n\nGiven a matrix V, and a new vector u, orthogonalize u  with respect to the columns of V, and computes its norm nu. If nu != 0, divides u by nu and returns  nu. If nu == 0, then u lives in the span of V, and 0 is returned.\n\n\n\n\n\n","category":"function"},{"location":"rbm_tutorial.html#Reduced-Basis-Method-Tutorial","page":"RBM Tutorial","title":"Reduced Basis Method Tutorial","text":"This tutorial follows closely to the book Reduced Basis Methods for Partial Differential Equations by Quateroni, Alfie, Manzoni, and Negri. For more information, see their text, source 2.","category":"section"},{"location":"rbm_tutorial.html#Problem-formulation-and-motivation","page":"RBM Tutorial","title":"Problem formulation and motivation","text":"In this tutorial, we consider scalar, linear, elliptic, parametrized PDEs of the form\n\nmathcalL(u(xp)p) = f(xp)\n\nwhere p is some parameter (vector), and the solution u depends on a spatial variable x and the parameter. We are interested in such problems specifically as upon discretization, say with finite elements, the discrete problem can be written in the form\n\nA(p) u(p) = b(p)\n\nwhere A(p)inmathbbR^Ntimes N, u(p)inmathbbR^N, and b(p)inmathbbR^N. Additionally, we will assume affine parameter dependence, i.e., we can write A(p) as \n\nA(p) = sum_i=1^QA theta_i^A(p) A_i\n\nand b(p) as\n\nb(p) = sum_i=1^Qb theta_i^b(p) b_i\n\nNote that if a problem does not match this form, there exist algorithms ((D)EIM) to convert the problem to this form.\n\nUpon sufficient discretization, we expect N to be large, and thus the problem of inverting A(p) several times for different parameter values can be expensive. A model order reduction technique is to build a reduced basis (RB) approximation to the solution. To do this, we wish to build an appropriate r dimensional RB space, with r ll N, on wish to use Galerkin projection. \n\nSpecifically, given linearly independent (assumed orthogonal) basis vectors to this space, v_i_i=1^r, we construct the RB space matrix\n\nV = beginbmatrix         v_1  v_2  cdots  v_r           endbmatrix in mathbbR^N times r\n\nsuch that the problem can be approximated by\n\nV^T A(p) V u_r(p) = V^T bquad u(p) approx V u_r(p)\n\nwhere now the task is to invert the much smaller, rtimes r matrix, V^T A(p) V to form u_r(p), and then the solution is approximated by V u_r(p). Additionally, due to the affine parameter dependence of A(p), we need not store any terms that depend on N, rather we only need to store the matrices V^T A_i V in mathbbR^rtimes r for i=1ldotsQA.\n\nNow, suppose we wished to solve an inverse problem, such as finding the parameter vector p^* that yields some some 'optimal' solution u^*(p). Or suppose that we wish to perform a sensitivity analysis of u(p) on several different parameter values p. These tasks would typically require us to solve the full-order problem a large number of times which may be computationally expensive. \n\nIf we are willing to spend offline time to generate an RB space, V, with dimension rll N, then we can much more efficiently spend time online computing the Galerkin projected solution, V u_r(p), at a fraction of a cost of computing the full-order solution.\n\nWe will consider a steady state heat equation for this tutorial with affine-parameter dependent spacial diffusion coefficient and forcing terms. Once discretized, it can be written in the form A(p) u = b(p) with A(p) and b(p) each with affine parameter dependence. This model can be instantiated in ModelOrderReductionToolkit.jl by calling PoissonModel().\n\nusing ModelOrderReductionToolkit\nmodel = PoissonModel()\n\nWe can then form a snapshot matrix over a set of P=125 parameter vectors.\n\nparams = [[i,j,k] for i in range(0,1,5) for j in range(0,1,5) for k in range(0,1,5)]\nP = length(params)\n\nS = zeros(output_length(model), P)\nfor i in 1:P\n    p = params[i]\n    u = model(p)\n    S[:,i] .= u\nend\n\nS\n\nLet's visualize the solutions.\n\nusing Plots\nplt = plot()\nfor i in 1:P\n    plot!(S[:,i],label=false,alpha=0.25)\nend\ntitle!(\"Solution Set\")\nsavefig(plt, \"rbm_tut1.svg\"); nothing # hide\n\n(Image: )","category":"section"},{"location":"rbm_tutorial.html#Proper-Orthogonal-Decomposition/Principal-Component-Analysis","page":"RBM Tutorial","title":"Proper Orthogonal Decomposition/Principal Component Analysis","text":"We know from the Schmidt-Eckart-Young theorem that the r-dimensional linear subspace that captures the most \"energy\" from the solutions in S (per the Frobenius norm) is the one spanned by the first r left singular vectors of S. More specifically, if we denote VinmathbbR^Ntimes r to be the matrix whose r columns are the first r left singular vectors of S, and let sigma_1geqsigma_2geqldotsgeqsigma_N be the singular values of S, then we can write that\n\nS - VV^TS_F = min_textrank(B)leq r A - B_F = sqrtsum_i=r+1^N sigma_i^2\n\nwhere VV^TS is the projection of S onto the columns of V.\n\nWe can explicitly compute the SVD and pull the first r columns as \n\nusing LinearAlgebra\nr = 5\nU,s,_ = svd(S)\nV = U[:,1:r]\nnothing; # hide\n\nWe can also plot the exponential singular value decay, suggesting to us that such an RBM will perform well.\n\nplt = plot(s, yaxis=:log, label=false)\nyaxis!(\"Singular Values\")\nxaxis!(\"Dimension\")\nsavefig(plt, \"rbm_tut2.svg\"); nothing # hide\n\n(Image: )\n\nNow, the Schmidt-Eckart-Young theorem tells us that this basis is optimal in the sense that it minimizes l^2 error in directly projecting our solutions, i.e., performing u(p) approx VV^Tu(p). Let's visualize the accuracy of these projections.\n\nplt = plot()\ncolors = palette(:tab10)\nidxs = [rand(1:P) for i in 1:6]\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(S[:,idx], c=colors[i], label=false)\n    u_approx = V * V' * S[:,idx]\n    plot!(u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected POD solutions\")\nsavefig(plt, \"rbm_tut3.svg\"); nothing # hide\n\n(Image: )\n\nHowever, we wished to create a reduced order model (ROM) such that given any new parameter value, we can quickly reproduce a new solution. As was noted before, we do this through a Galerkin projection\n\nV^T A(p) V u_r(p) = V^T b implies u(p) approx V u_r(p) = u_textapprox(p)\n\nfrom which we require only inverting an rtimes r matrix. Although this is no longer guaranteed \"optimal\" by the Schmidt-Eckart-Young theorem, let's see how this performs on the same snapshots. To perform this task in ModelOrderReductionToolkit.jl, we pass the snapshot matrix into a PODReductor object and form a ROM from the reductor.\n\npod_reductor = PODReductor(model)\nadd_to_rb!(pod_reductor, S)\npod_rom = form_rom(pod_reductor, r)\nplt = plot()\ncolors = palette(:tab10)\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(S[:,idx], c=colors[i], label=false)\n    u_r = pod_rom(p)\n    u_approx = lift(pod_reductor, u_r)\n    plot!(u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected Galerkin POD solutions\")\nsavefig(plt, \"rbm_tut4.svg\"); nothing # hide\n\n(Image: )\n\nAs we can see from these plots, a 5-dimensional approximation is quite accurate here! Even though after discretization, these solutions lie in mathbbR^999, we have shown that the solution manifold lies approximately on a 5-dimensional space. Additionally, even though we were only guaranteed \"optimality\" from direct projection of solutions, we still have very good accuracy when we use a Galerkin projection on the problem.\n\nThis process of projection onto left singular values is typically called Proper Orthogonal Decomposition (POD). Note that forming a PODReductor will call svd on the snapshot matrix. We can access the singular values from pod_reductor.S, and the left-singular vectors from pod_reductor.V (note that left-singular vectors are usually denoted with U, but we use V to stick with RB notation).","category":"section"},{"location":"rbm_tutorial.html#Strong-Greedy-Algorithm","page":"RBM Tutorial","title":"Strong Greedy Algorithm","text":"An alternative way to generate this reduced basis is through a process called the strong greedy algorithm. This algorithm is called greedy, because we iteratively choose basis elements in a greedy way. We begin by choosing v_1 to be the column of our solution matrix, S with the largest norm, and then normalized it by its length\n\ns_1^* = max_i s_iquad v_1 = fracs_1^*s_1^*\n\nNow, we use a Gram-Schmidt procedure to orthogonalize all other columns of S with respect to v_1:\n\ns_i^(1) = s_i - (v_1^T s_i) v_1quad i=1ldotsP\n\nAfter the j-1'st element v_j-1 is chosen and all of the orthogonalization is performed, we then choose v_j to be the column of S^(j-1) which has the largest norm, i.e., has the worst projection error:\n\ns_j^* = max_i s_i^(j-1)quad v_j = fracs_j^*s_j^*\n\nand again orthogonalize\n\ns_i^(j) = s_i^(j-1) - (v_j^T s_i^(j-1)) v_jquad i=1ldotsP\n\nNote that this procedure is exactly like performing a pivoted QR factorization on the matrix S. Let's form this reduced basis of the same dimension:\n\nQ,_,_ = qr(S, LinearAlgebra.ColumnNorm())\nV = Q[:,1:r]\nnothing; # hide\n\nNow, we will play the same game. First, we directly project the solutions onto this space\n\nplt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(S[:,idx], c=colors[i], label=false)\n    u_approx = V * V' * S[:,idx]\n    plot!(u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected SG solutions\")\nsavefig(plt, \"rbm_tut5.svg\"); nothing # hide\n\n(Image: )\n\nNow, we will use an SGReductor object to form a Galerkin-projected reduced order model.\n\nsg_reductor = SGReductor(model)\nadd_to_rb!(sg_reductor, S)\nsg_rom = form_rom(sg_reductor, r)\nplt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(S[:,idx], c=colors[i], label=false)\n    u_r = sg_rom(p)\n    u_approx = lift(sg_reductor, u_r)\n    plot!(u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected Galerkin SG solutions\")\nsavefig(plt, \"rbm_tut6.svg\"); nothing # hide\n\n(Image: )\n\nThis procedure also performs quite well. We may expect the POD algorithm to be a bit more accurate/general as it can choose basis elements that are not \"in the columns\" of S. Similar to the PODReductor object, we can access the reduced basis from sg_reductor.V.","category":"section"},{"location":"rbm_tutorial.html#Weak-Greedy-Algorithm","page":"RBM Tutorial","title":"Weak Greedy Algorithm","text":"Now, one downside of the above procedures was that we needed the matrix of full-order solutions ahead of time to perform either the SVD or QR factorizations. If our model was very computationally expensive, we would not want to have to do this. This is where the weak greedy algorithm is useful. It is again a greedy algorithm as we will be choosing \"columns\" greedily, but we wish to not have to construct all columns directly.\n\nSuppose now, instead of having access to the columns s_i which correspond to the full-order solutions u(p_i), we only have access to the parameter values p_i. Generally, we would wish to have an a priori error estimator for a Galerkin-projected ROM such that we could loop over our parameter vectors and choose the one with the estimated maximum error. In this tutorial, we use a stability-residual approach\n\nOne can show that there exists an upper-bound on projection error, given by\n\nu(p) - V u_r(p) = A(p)^-1 b(p) - V u_r(p) leq fracb(p) - A(p) V u_r(p)sigma_min(A(p))\n\nwhere sigma_min(A(p)) is the minimum singular value of A(p). Note that this upperbound on the error does not depend on the full order solution, u(p). So, we loop through each parameter vector p_i, and select the one, p^* that yields the highest upper-bound error. We then form the full-order solution u(p_i), normalize it, and append it as a column of V. Note that unlike in the strong algorithm, since we are not using true error, we are not guaranteed to choose the next \"best\" column of V. However, if we are computing a reduced basis of size r, then we only need to call the full-order model r times.\n\nWe now need a method to approximate (a lowerbound of ) sigma_min(A(p)), and then the numerator of the above can be computed explicitly. One way of doing this is through the successive constraint method (SCM). This method takes advantage of the affine parameter dependence of A(p), see source 1. We will form an SCM object and initialize an object to compute the norm of the residual through a StabilityResidualErrorEstimator.\n\nscm = SCM(model.Ap, params, coercive=true)\nerror_estimator = StabilityResidualErrorEstimator(model, scm)\nnothing # hide\n\nNote that this method assumes that the model is coercive, i.e., the matrix A(p) is symmetric positive definite for each parameter. For this model, we know that this is the case if the parameter vectors have entries between 0 and 1. For a noncoercive model, add the keyword argument coercive=false. With this in place, we have enough to construct the weak greedy reductor.\n\nwg_reductor = WGReductor(model, error_estimator)\n\nNote that we have to build the reduced basis by looping over a parameter set, we will do this by calling add_to_rb!. Afterwards, since the reductor must store the ROM at each step to make error approximations, we can simply pull it from the reductor object.\n\nadd_to_rb!(wg_reductor, params, r)\nprintln(\" \") # hide\n\nwg_rom = wg_reductor.rom\n\nWe can access the greedily chosen reduced basis by calling (note that for computational purposes, V is stored as a VectorOfVectors object).\n\nwg_reductor.V\n\nWe can now visualize these solutions by calling wg_rom(p) on a paramater vector p.\n\nplt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(S[:,idx], c=colors[i], label=false)\n    u_r = wg_rom(p)\n    u_approx = lift(wg_reductor, u_r)\n    plot!(u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and WG solutions\")\nsavefig(plt, \"rbm_tut7.svg\"); nothing # hide\n\n(Image: )","category":"section"},{"location":"rbm_tutorial.html#Comparison-of-the-methods","page":"RBM Tutorial","title":"Comparison of the methods","text":"Let's compare the above algorithms by comparing their average and worst case accuracy over the parameter set.\n\nerrors = [Float64[] for _ in 1:3]\nfor (i,p) in enumerate(params)\n    pod_error = norm(lift(pod_reductor, pod_rom(p)) .- S[:,i])\n    push!(errors[1], pod_error)\n    sg_error = norm(lift(sg_reductor, sg_rom(p)) .- S[:,i])\n    push!(errors[2], sg_error)\n    wg_error = norm(lift(wg_reductor, wg_rom(p)) .- S[:,i])\n    push!(errors[3], wg_error)\nend\nprintln(\"Errors for RB dimension r=$r\")\nprintln(\"POD mean error: $(sum(errors[1]) / length(errors[1]))\")\nprintln(\"POD worst error: $(maximum(errors[1]))\")\nprintln(\"SG mean error: $(sum(errors[2]) / length(errors[2]))\")\nprintln(\"SG worst error: $(maximum(errors[2]))\")\nprintln(\"WG mean error: $(sum(errors[3]) / length(errors[3]))\")\nprintln(\"WG worst error: $(maximum(errors[3]))\")\nnothing # hide\n\nLet's repeat the process for a reduced basis of dimension r=15 Let's compare the above algorithms by comparing their average and worst case accuracy over the parameter set.\n\noldr = r\nr = 15\npod_rom = form_rom(pod_reductor, r)\nsg_rom = form_rom(sg_reductor, r)\nadd_to_rb!(wg_reductor, params, r - oldr)\nwg_rom = wg_reductor.rom\nerrors = [Float64[] for _ in 1:3]\nfor (i,p) in enumerate(params)\n    pod_error = norm(lift(pod_reductor, pod_rom(p)) .- S[:,i])\n    push!(errors[1], pod_error)\n    sg_error = norm(lift(sg_reductor, sg_rom(p)) .- S[:,i])\n    push!(errors[2], sg_error)\n    wg_error = norm(lift(wg_reductor, wg_rom(p)) .- S[:,i])\n    push!(errors[3], wg_error)\nend\nprintln(\"Errors for RB dimension r=$r\")\nprintln(\"POD mean error: $(sum(errors[1]) / length(errors[1]))\")\nprintln(\"POD worst error: $(maximum(errors[1]))\")\nprintln(\"SG mean error: $(sum(errors[2]) / length(errors[2]))\")\nprintln(\"SG worst error: $(maximum(errors[2]))\")\nprintln(\"WG mean error: $(sum(errors[3]) / length(errors[3]))\")\nprintln(\"WG worst error: $(maximum(errors[3]))\")\nnothing # hide\n\nIn conclusion, the weak greedy algorithm takes advantage of the affine parameter dependence of A(p) and b(p), and uses an upper-bound error approximator to produce a reduced basis that approximates solutions with comparable error compared to the strong greedy algorithm and the POD algorithm without needing to compute all solutions ahead of time.","category":"section"},{"location":"rbm_tutorial.html#References:","page":"RBM Tutorial","title":"References:","text":"D.B.P. Huynh, G. Rozza, S. Sen, A.T. Patera. A successive constraint linear optimization method for lower bounds of parametric coercivity and inf–sup stability constants. Comptes Rendus Mathematique. Volume 345, Issue 8. 2007. Pages 473-478. https://doi.org/10.1016/j.crma.2007.09.019.\nQuarteroni, Alfio, Andrea Manzoni, and Federico Negri. Reduced Basis Methods for Partial Differential Equations. Vol. 92. UNITEXT. Cham: Springer International Publishing, 2016. http://link.springer.com/10.1007/978-3-319-15431-2.","category":"section"}]
}
