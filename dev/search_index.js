var documenterSearchIndex = {"docs":
[{"location":"test_prob.html#Test-Problem","page":"Test Problem","title":"Test Problem","text":"","category":"section"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"Consider the parametrized PDE problem","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"begincases-fracddx left(kappa(xp) fracddx u(xp)right) = f(xp)quad xin01\nu(0p) = 00\nu(1p) = 10\n01leq p_1p_2leq 21\nendcases","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"where ","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"kappa(xp) = sum_i=1^2 p_i cdot (11 + sin(2pi i x))","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"and ","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"f(xp) = begincases100  x  05  00  x leq 05endcasesbigg + sum_i=1^2 p_i cdot sin(2pi i x)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"are affinely parameter dependent diffusion coefficient and forcing terms respectively. ","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"We can discretize and explicitly solve this problem by using finite differences, the problem can be rewritten as","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"A(p) u(p) = b(p)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"where u(p)inmathbbR^n,  A(p)inmathbbR^ntimes n is a positive-definite matrix, and both A(p) and b(p)inmathbbR^n have affine parameter dependence.","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"using ModelOrderReductionToolkit\nusing Plots\nusing Printf\nusing Random\nusing SparseArrays\nusing Colors\ngr() # hide\nRandom.seed!(1) # hide\n# Boundary conditions\nuleft = 0.0\nuright = 1.0\nP = 2\nκ_i(i,x) = 1.1 .+ sin.(2π * i * x)\nκ(x,p) = sum([p[i] * κ_i(i,x) for i in 1:P])\n# Parameter dependent source term\nf_i(i,x) = i == 1 ? (x .> 0.5) .* 10.0 : 20 .* sin.(2π * (i-1) * x)\nf(x,p) = f_i(1,x) .+ sum([p[i-1] * f_i(i,x) for i in 2:P+1])\n# Space setup\nh = 1e-3\nxs = Vector((0:h:1)[2:end-1])\nxhalfs = ((0:h:1)[1:end-1] .+ (0:h:1)[2:end]) ./ 2\nN = length(xs)\n# Helper to generate random parameter vector\nrandP() = 0.1 .+ 2 .* rand(P)\n# Ai matrices\nfunction makeAi(i)\n    A = spzeros(N,N)\n    for j in 1:N\n        A[j,j]   = (κ_i(i, xhalfs[j]) + κ_i(i, xhalfs[j+1])) / h^2\n        if j<N\n            A[j,j+1] = -1 * κ_i(i, xhalfs[j+1]) / h^2\n        end\n        if j>1\n            A[j,j-1] = -1 * κ_i(i, xhalfs[j]) / h^2\n        end\n    end\n    return A\nend\nAis = Matrix{Float64}[]\nfor i in 1:P\n    push!(Ais, makeAi(i))\nend\n# θAis\nfunction makeθAi(p,i)\n    return p[i]\nend\nfunction makeA(p)\n    A = zeros(size(Ais[1]))\n    for i in eachindex(Ais)\n        A .+= makeθAi(p,i) .* Ais[i]\n    end\n    return A\nend\n# bi vectors\nfunction makebi(i)\n    b = f_i(i,xs)\n    if i > 1\n            b[1] += uleft * κ_i(i, xhalfs[1]) / h^2\n            b[end] += uright * κ_i(i, xhalfs[end]) / h^2\n        end\n    return b\nend\nbis = Vector{Float64}[]\nfor i in 1:P+1\n    push!(bis, makebi(i))\nend\n# θbis\nfunction makeθbi(p,i)\n    if i == 1\n        return 1.0\n    else\n        return p[i-1]\n    end\nend\nfunction makeb(p)\n    b = zeros(size(bis[1]))\n    for i in eachindex(bis)\n        b .+= makeθbi(p,i) .* bis[i]\n    end\n    return b\nend\n\nplt = plot(legend=:topleft)\ncolors = palette(:tab10)\nps = []\nfor i in 1:10\n    p = randP()\n    push!(ps, p)\n    A = makeA(p)\n    b = makeb(p)\n    u = A \\ b\n    plot!(xs, u, label=@sprintf(\"p=[%.2f,%.2f]\",p...), c=colors[i])\nend\ntitle!(\"Example solutions\")\nsavefig(plt, \"ex1.svg\"); nothing # hide","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"(Image: )","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"We will construct a reduced basis method (RBM) to construct an r-dimensional solution space to construct approximations","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"u(p) approx V u_r(p) = sum_i=1^r u_r^(i)(p) v_i","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"where v_i_i=1^r is a chosen basis with u_r(p)inmathbbR^r, VinmathbbR^ntimes r, and rll n. Additionally, due to the potential computational cost of inverting large A(p), we wish to minimize the number of times we solve the full-order, truth, system.","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"Defining e(p)=u(p) - V u_r(p) to be the error in our approximation, and r(p)=b(p) - A(p) V u_r(p) to be the residual, one can show that","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"e(p)_2 = A(p)^-1 A(p)(u(p) - V u_r(p))_2 leq r(p)_2  sigma_min(A(p))","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"The goal of this method will be to use offline time to approximate the stability factor, sigma_min(A(p)), and the norm of the residual, r(p)_2.","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"First, we will use the successive constraint method, (SCM), to compute a lower-bound approximation of the stability factor, to assure that the above error inequality still holds. For details on implementation, see reference 1. In ModelOrderReductionToolkit.jl, the implementation for a symmetric positive definite problem is as follows:","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"# Generate parameter discretization set\np_lb = 0.1\np_ub = 2.1\nparams = []\nfor p1 in range(p_lb,p_ub,21)\n    for p2 in range(p_lb,p_ub,21)\n        push!(params, [p1,p2])\n    end\nend\n# Initialize SCM Object\nMa = 50; Mp = 15; ϵ_SCM = 1e-2;\nscm = initialize_SCM_SPD(params, Ais, makeθAi, Ma, Mp, ϵ_SCM)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"As can be seen in the below example, the method find_sigma_bounds(scm,p) returns both an upper-bound and a lower-bound estimate for the stability factor with a relative error on the order of ϵ_SCM as long as it is not too far from a parameter value in the discretization, params.","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"randp = randP()\nfind_sigma_bounds(scm, randp)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"Can also directly use scm as a function to approximate the lower bound","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"scm(randp)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"Note that in the case that A(p) is not SPD, one should instead use the noncoercive method.","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"scm = initialize_SCM_Noncoercive(params, Ais, makeθAi, Ma, Mp, ϵ_SCM)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"With the SCM object initialized, one can call the GreedyRBAffineLinear method to greedily generate a reduced basis, v_i_i=1^r, one element at a time, to an l^2 error on the order of epsilon_greedy, see reference 2,","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"ϵ_greedy = 1e-1  \ngreedy_sol = GreedyRBAffineLinear(params, Ais, makeθAi, bis, makeθbi, scm, ϵ_greedy)","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"plt = plot(legend=:topleft)\nfor i in 1:10\n    p = ps[i]\n    A = makeA(p)\n    b = makeb(p)\n    u = A \\ b\n    u_approx = greedy_sol(p)\n    plot!(xs, u, c=colors[i],label=(i==1 ? \"truth\" : false))\n    plot!(xs, u_approx, c=colors[i], ls=:dash,label=(i==1 ? \"greedy\" : false))\nend\ntitle!(\"Example solutions and greedy solutions\")\nsavefig(plt, \"ex2.svg\"); nothing # hide","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"(Image: )","category":"page"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"Note that the method GreedyRBAffineLinear takes advantage of the affine parameter dependence of both the left and right hand sides for quick evaluation (approximation) of the norm of the residual, and the stability factor, which does not depend on the dimension of the original problem n. Only after it has looped through all of the parameters in the discretization, it chooses the one that displays the worst upper-bound error, computes the full-order solution for that parameter, and adds that solution as the next reduced basis element, v_i.","category":"page"},{"location":"test_prob.html#References:","page":"Test Problem","title":"References:","text":"","category":"section"},{"location":"test_prob.html","page":"Test Problem","title":"Test Problem","text":"D.B.P. Huynh, G. Rozza, S. Sen, A.T. Patera. A successive constraint linear optimization method for lower bounds of parametric coercivity and inf–sup stability constants. Comptes Rendus Mathematique. Volume 345, Issue 8. 2007. Pages 473-478. https://doi.org/10.1016/j.crma.2007.09.019.\nQuarteroni, Alfio, Andrea Manzoni, and Federico Negri. Reduced Basis Methods for Partial Differential Equations. Vol. 92. UNITEXT. Cham: Springer International Publishing, 2016. http://link.springer.com/10.1007/978-3-319-15431-2.","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.jl-Docstrings","page":"Docstrings","title":"ModelOrderReductionToolkit.jl Docstrings","text":"","category":"section"},{"location":"index.html#Projections-functionality:","page":"Docstrings","title":"Projections functionality:","text":"","category":"section"},{"location":"index.html","page":"Docstrings","title":"Docstrings","text":"singular_values_information\npca_projector\nqr_projector\nModelOrderReductionToolkit.full_lu\neim_projector","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.singular_values_information","page":"Docstrings","title":"ModelOrderReductionToolkit.singular_values_information","text":"singular_values_information(σs)\n\nGiven a vector of real singular values σs, return a vector providing the maximum percentage of \"information\" available from a d-dimensional representation in Frobenius norm.\n\nUses the fact that the maximum error in projecting onto the first d left singular vectors is given by\n\n||A-A_d||_F = √ (σ_{d+1}^2 + ... + σ_N^2).\n\nSo the information with a d-dimensional representation given by\n\nsingular_values_information[d] = 1 - √ (σ_{d+1}^2 + ... + σ_N^2) / √ (σ_1^2 + ... + σ_N^2)\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.pca_projector","page":"Docstrings","title":"ModelOrderReductionToolkit.pca_projector","text":"P = pca_projector(A, dim=-1; info_tol=1e-2)\n\nUses the first dim singular vectors of A to generate an orthogonal projection operator P. Operator performs matrix operation and can be done on vectors or matrices of proper dimension.\n\nIf dim is not provided, uses n singular values  where n is the smallest integer such that the amount of information per the Frobenious norm is at least 1-info_tol.\n\nReturns a functor P which stores the reduced dimension n, the dimension of vectors to project m, the singular values of A in s, and the vectors to project onto in the m × n matrix M.\n\nCall with P(b) where b is a m vector or a matrix with m rows, projecting each column.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.qr_projector","page":"Docstrings","title":"ModelOrderReductionToolkit.qr_projector","text":"qr_projector(A, dim)\n\nUses the first dim columns of A by qr-pivot order (norm) to generate an orthogonal projection operator P. Operator performs matrix operation and can be done on vectors or matrices of proper dimension.\n\nReturns a functor P which stores the reduced dimension n, the dimension of vectors to project m, the vectors  to project onto in the m × n matrix M, and the matrix inv(M'M) in MtMinv.\n\nCall with P(b) where b is a m vector or a matrix with m rows, projecting each column.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.full_lu","page":"Docstrings","title":"ModelOrderReductionToolkit.full_lu","text":"full_lu(A; steps=-1)\n\nPerforms a completely pivoted LU factorization on the matrix A, returning permutation vectors Q and P, and lower and upper triangular matrices L and U, such that if all steps are performed, then A[P,Q] = L*U.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.eim_projector","page":"Docstrings","title":"ModelOrderReductionToolkit.eim_projector","text":"P = eim_projector(A, dim)\n\nUses discrete empirical interpolation to generate a projector operator P that selects a few rows from the given vector or matrix, and then projects them onto a set of columns chosen by a greedy procedure.\n\nReturns a functor P which stores the reduced dimension n, the dimension of vectors to project m, the matrix A_C which is A with n extracted columns, and the matrix A_CR which is A with n extracted rows and columns.\n\nCall with P(b) where b is a m vector or a matrix with m rows, projecting each column.\n\n\n\n\n\n","category":"function"},{"location":"index.html#Greedy-linear-affine-reduced-basis","page":"Docstrings","title":"Greedy linear affine reduced basis","text":"","category":"section"},{"location":"index.html","page":"Docstrings","title":"Docstrings","text":"ModelOrderReductionToolkit.Greedy_RB_Affine_Linear\nGreedyRBAffineLinear\nModelOrderReductionToolkit.greedy_rb_err_data\nModelOrderReductionToolkit.append_affine_rbm!\nModelOrderReductionToolkit.init_affine_rbm","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.Greedy_RB_Affine_Linear","page":"Docstrings","title":"ModelOrderReductionToolkit.Greedy_RB_Affine_Linear","text":"Greedy_RB_Affine_Linear\n\nStruct for containing initialized greedy reduced-basis method for parametrized problem A(p) u = b(p) with affine parameter dependence A(p) = ∑ makeθAi(p,i) Ais[i], and b(p) = ∑ makeθbi(p,i) bis[i].\n\nUses Galerkin projection onto span of columns of V,  V' A(p) V u_r = V' b(p), with V u_r ≈ u = A(p)^(-1) b(p).\n\nGiven a new parameter vector, p, and an object greedy_sol::Greedy_RB_Affine_Linear, form the reduced basis solution with greedy_sol(p[, full=true]). \n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.GreedyRBAffineLinear","page":"Docstrings","title":"ModelOrderReductionToolkit.GreedyRBAffineLinear","text":"GreedyRBAffineLinear(param_disc, Ais, makeθAi, bis, makeθbi, approx_stability_factor[, ϵ=1e-2, param_disc=nothing; max_snapshots=-1, noise=1])\n\nConstructs a Greedy_RB_Affine_Linear object for parametrized problem A(p) y(p) = b(p) with affine parameter dependence: A(p) = ∑ makeθAi(p,i) Ais[i], and b(p) = ∑ makeθbi(p,i) bis[i].\n\nparam_disc must either be a matrix with columns as parameters, or a vector of parameter vectors. Uses the function approx_stability_factor to approximate the stability factor for each parameter in the discretization,  chooses first parameter to be one with smallest stability factor. Afterwards, utilizes the affine decomposition of A(p)  and b(p) to compute the norm of the residual, and compute upper-bounds for the error: ||u - V u_r|| <= ||b(p) - A(p) V u_r|| / σ_min(A(p)).\n\nGreedily chooses parameters and then forms truth solution for parameter that results in largest upper-bound for error. Then computes ||u - V u_r||, and loops, adding to reduced basis, V, until the truth error is less than ϵ.\n\nIf max_snapshots specified, will halt after that many if ϵ accuracy not yet reached. noise determines amount  of printed output, 0 for no output, 1 for basic, and 2 for more.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.greedy_rb_err_data","page":"Docstrings","title":"ModelOrderReductionToolkit.greedy_rb_err_data","text":"greedy_rb_err_data(param_disc,Ais,makeθAi,bis,makeθbi,approx_stability_factor[,num_snapshots=10,ϵ=1e-2;noise=1)\n\nGenerates error data for parametrized problem A(p) u(p) = b(p) with affine parameter dependence: A(p) = ∑ makeθAi(p,i) Ais[i], and b(p) = ∑ makeθbi(p,i) bis[i].\n\nNote: This method calls the full order solver on all parameters in param_disc, may take a long time to run.\n\nSee the docs for GreedyRBAffineLinear for more information.\n\nReturns a dictionary, ret_data, with the following components:\n\nret_data[:basis_dim] - A vector of reduced basis dimensions from 2 to num_snapshots\n\nret_data[:weak_greedy_ub] - A vector of the maximum upperbound l2 error found by the  (weak) greedy reduced basis method (see GreedyRBAffineLinear)\n\nret_data[:weak_greedy_true] - A vector of the truth l2 error of the vector chosen by the  (weak) greedy reduced basis method (see GreedyRBAffineLinear)\n\nret_data[:weak_greedy_true_ub] - A vector of the maximum true l2 error found by the  (weak) greedy reduced basis method (see GreedyRBAffineLinear)\n\nret_data[:strong_greedy_err] - A vector of the maximum l2 error found by a   strong greedy reduced basis method, uses knowledge of all solutions\n\nret_data[:strong_greedy_proj] - A vector of the maximum l2 error found by projecting   truth solutions onto the strong greedy reduced basis, uses knowledge of all solutions\n\nret_data[:pca_err] - A vector of the maximum l2 error found by a PCA/POD reduced basis method, uses knowledge of all solutions\n\nret_data[:strong_greedy_proj] - A vector of the maximum l2 error found by projecting   truth solutions onto the PCA/POD reduced basis, uses knowledge of all solutions\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.append_affine_rbm!","page":"Docstrings","title":"ModelOrderReductionToolkit.append_affine_rbm!","text":"append_affine_rbm!(x, res_init, V, Ais, VtAVis, bis, Vtbis)\n\nGiven a new vector x to append as a new column to V, update the matrices in VtAVis and the vectors in Vtbis. Also, return VtAV and Vtb which are preallocated, properly sized, matrices. Must pass in type T.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.init_affine_rbm","page":"Docstrings","title":"ModelOrderReductionToolkit.init_affine_rbm","text":"init_affine_rbm(x, Ais, bis, makeθAi, makeθbi, T)\n\nGiven a vector x, generate the matrices V, VtAVi, and VtAVis, and the vectors Vtb and Vtbis for preallocation and quick computation of reduced basis solutions for the problem A(p)x(p)=b(p) with affinely dependent matrix A(p) = ∑ makeθAi(p,i) Ais[i] and vector b(p) = ∑ makeθbi(p,i) bis[i]. Must pass in type T.\n\n\n\n\n\n","category":"function"},{"location":"index.html#Successive-constraint-method-(SCM):","page":"Docstrings","title":"Successive constraint method (SCM):","text":"","category":"section"},{"location":"index.html","page":"Docstrings","title":"Docstrings","text":"ModelOrderReductionToolkit.SCM_Init\ninitialize_SCM_SPD\ninitialize_SCM_Noncoercive\nfind_sigma_bounds\nModelOrderReductionToolkit.form_upperbound_set!\nModelOrderReductionToolkit.solve_LBs_LP\nModelOrderReductionToolkit.smallest_real_eigval\nModelOrderReductionToolkit.largest_real_eigval\nModelOrderReductionToolkit.smallest_real_pos_eigpair","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.SCM_Init","page":"Docstrings","title":"ModelOrderReductionToolkit.SCM_Init","text":"SCM_Init is a struct for holding all of the variables and methods necessary for running the successive constraint method on an affinely-parameter-dependent matrix A(p) = ∑_{i=1}^QA θ_i(p) A_i to compute a lower-bound approximation to the minimum singular value of A.\n\nIt is additionally a functor as calling scm_init(p) on a parameter vector p will return the lower-bound estimate of the minimum singular value of A(p).\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.initialize_SCM_SPD","page":"Docstrings","title":"ModelOrderReductionToolkit.initialize_SCM_SPD","text":"initialize_SCM_SPD(param_disc,Ais,makeθAi,Mα,Mp,ϵ;[optimizer=Tulip.Optimizer,noise=1]) = SCM_Init\n\nMethod to initialize an SCM_Init object to perform the SCM on an affinely-parameter-dependent symmetric positive definite matrix A(p) = ∑ makeθAi(p,i) Ais[i] to compute a lower-bound approximation to the minimum singular value (or eigenvalue) of A(p).\n\nParameters:\n\nparam_disc: Either a matrix where each column is a parameter value in the discretization, or a vector of parameter vectors.\n\nAis: A vector of matrices, of the same dimension, used to construct the full matrix A(p) = ∑ makeθAi(p,i) Ais[i]\n\nmakeθAi(p,i): A function that takes in as input a parameter vector and an index such that A(p) = ∑ makeθAi(p,i) Ais[i]\n\nMα: Stability constraint constant (a positive integer)\n\nMp: Positivity constraint constant (a positive integer)\n\nϵ: Relative difference allowed between upper-bound and lower-bound approximation on the parameter discretization (between 0 and 1)\n\noptimizer: Optimizer to pass into JuMP Model method for solving linear programs\n\nnoise: Determines amount of printed information, between 0 and 2 with 0 being nothing displayed; default 1\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.initialize_SCM_Noncoercive","page":"Docstrings","title":"ModelOrderReductionToolkit.initialize_SCM_Noncoercive","text":"initialize_SCM_Noncoercive(param_disc,Ais,makeθAi,Mα,Mp,ϵ;[optimizer=Tulip.Optimizer,noise=1]) = SCM_Init\n\nMethod to initialize an SCM_Init object to perform the SCM on an affinely-parameter-dependent matrix A(p) = ∑ makeθAi(p,i) Ais[i] to compute a lower-bound approximation to the minimum singular value of A.\n\nParameters:\n\nparam_disc: Either a matrix where each column is a parameter value in the discretization, or a vector of parameter vectors.\n\nAis: A vector of matrices, of the same dimension, used to construct the full matrix A(p) = ∑ makeθAi(p,i) Ais[i]\n\nmakeθAi(p,i): A function that takes in as input a parameter vector and an index such that A(p) = ∑ makeθAi(p,i) Ais[i]\n\nMα: Stability constraint constant (a positive integer)\n\nMp: Positivity constraint constant (a positive integer)\n\nϵ: Relative difference allowed between upper-bound and lower-bound approximation on the parameter discretization (between 0 and 1)\n\noptimizer: Optimizer to pass into JuMP Model method for solving linear programs\n\nnoise: Determines amount of printed information, between 0 and 2 with 0 being nothing displayed; default 1\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.find_sigma_bounds","page":"Docstrings","title":"ModelOrderReductionToolkit.find_sigma_bounds","text":"find_sigma_bounds(scm_init, p[, sigma_eps=1.0; noise=0])\n\nMethod that performs the online phase of SCM for the matrix A(p) = ∑ makeθAi(p,i) Ais[i] to compute lower and upper-bound approximations to the minimum singular value of A. Additional optional parameter sigma_eps such that if the computed ϵ difference of (σ_UB - σ_LB) / σ_UB is less than sigma_eps,  we know that not enough stability constraints were enforced, and  the minimum singular value is directly computed, appended to the  scm_init's upper-bound set, and returned as both the lower and upper-bounds.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.form_upperbound_set!","page":"Docstrings","title":"ModelOrderReductionToolkit.form_upperbound_set!","text":"form_upperbound_set!(scm_init; [noise=1])\n\nHelper method that takes in an SCM_Init_SPD object and forms the upper-bound sets C, Y_UB, and σ_UBs until the ϵ tolerance provided in scm_init is met across the parameter discretization.  The noise input determines the  amount of printed information, between 0 and 2, with 0 being nothing displayed, and 1 is default.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.solve_LBs_LP","page":"Docstrings","title":"ModelOrderReductionToolkit.solve_LBs_LP","text":"solve_LBs_LP(scm_init, p[; noise=1]) = (σ_LB, y_LB)\n\nHelper method that takes in an SCM_Init_SPD object and a parameter vector p, and sets up and solves a linear program to compute a lower-bound σ_LB to the minimum singular value of A(p) along with the associated vector y_LB.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.smallest_real_eigval","page":"Docstrings","title":"ModelOrderReductionToolkit.smallest_real_eigval","text":"smallest_real_eigval(A, kmaxiter[, noise=1, krylovsteps=10])\n\nGiven a hermitian matrix A, attempts to compute the  most negative (real) eigenvalue. First, uses Krylov iteration with a shift-invert procedure with Gershgorin disks, and if  not successful, calls a full, dense, eigensolve.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.largest_real_eigval","page":"Docstrings","title":"ModelOrderReductionToolkit.largest_real_eigval","text":"largest_real_eigval(A, kmaxiter[, noise=1])\n\nGiven a hermitian matrix A, attempts to compute the  most positive (real) eigenvalue. First, uses Krylov iteration with no shift-invert, and if not successful, calls a full,  dense, eigensolve.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.smallest_real_pos_eigpair","page":"Docstrings","title":"ModelOrderReductionToolkit.smallest_real_pos_eigpair","text":"smallest_real_pos_eigpair(A, kmaxiter[, noise=1])\n\nGiven a hermitian, positive definite matrix A, attempts to compute the  smallest (real) eigenvalue and eigenvector. First, uses Krylov iteration with shift-invert around zero, and if not successful, calls a full,  dense, eigensolve. Returns a tuple with the first component being the  eigenvalue, and the second component being the eigenvector.\n\n\n\n\n\n","category":"function"},{"location":"index.html#Radial-basis-interpolatory-stability-factor","page":"Docstrings","title":"Radial-basis interpolatory stability factor","text":"","category":"section"},{"location":"index.html","page":"Docstrings","title":"Docstrings","text":"ModelOrderReductionToolkit.Sigma_Min_RBF\nmin_sigma_rbf\nupdate_sigma_rbf!","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.Sigma_Min_RBF","page":"Docstrings","title":"ModelOrderReductionToolkit.Sigma_Min_RBF","text":"Sigma_Min_RBF\n\nA mutable struct used for approximating the minimum singular value of a matrix  with parametric dependence A(p). Given  a new parameter value p, approximate σ_min(A(p)) with sigma_min_rbf(p).\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.min_sigma_rbf","page":"Docstrings","title":"ModelOrderReductionToolkit.min_sigma_rbf","text":"min_sigma_rbf(params, Ais, makeθAi[, ϕ=gaussian_rbf])\n\nMethod to form a interpolatory radial-basis function functor to approximate the minimum singular value of a parametrized matrix A(p). Pass as input either a matrix params where each column is a parameter vector, or a vector  of vectors params where each vector is a parameter. \n\nOptional argument for the radial-basis function ϕ, which defaults to the Gaussian ϕ(r) = exp(-r^2).\n\nReturns a functor sigma_min_rbf <: Sigma_Min_RBF such that given a new parameter vector p, sigma_min_rbf(p) returns an approximation to the minimum singular value of A(p).\n\nOffline, solves for the minimum singular value for each parameter in params, and uses this to form an interpolatory approximation in the form of log(σ_min(A(p))) ≈ ω_0 + ∑ (ω_i p_i) + ∑ (γ_i ϕ(p - p_i)) where p_i is the i'th parameter in params, and ω_0, ω_i, and γ_i are determined by the given params such that the approximation holds true for all p_i, and that ∑ γ_i = 0, and that ∑ γ_i p_i[j] = 0 for each j.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.update_sigma_rbf!","page":"Docstrings","title":"ModelOrderReductionToolkit.update_sigma_rbf!","text":"update_sigma_rbf!(sigma_min_rbf, ϕ)\n\nGiven a Sigma_Min_RBF object sigma_min_rbf, update its RBF, ϕ, and update the coefficients.\n\n\n\n\n\n","category":"function"},{"location":"index.html#Computation-of-norm-of-residual","page":"Docstrings","title":"Computation of norm of residual","text":"","category":"section"},{"location":"index.html","page":"Docstrings","title":"Docstrings","text":"ModelOrderReductionToolkit.Affine_Residual_Init\nresidual_norm_affine_init\nadd_col_to_V!\nresidual_norm_affine_online\nresidual_norm_explicit","category":"page"},{"location":"index.html#ModelOrderReductionToolkit.Affine_Residual_Init","page":"Docstrings","title":"ModelOrderReductionToolkit.Affine_Residual_Init","text":"Affine_Residual_Init\n\nA struct for containing the necessary vectors and matrices for quickly compute the X-norm of the residual, r(u_r,p) = A(p) (u - V u_r) = b(p) - A(p) V u_r, by taking advantage of affine parameter dependence of A(p) and b(p).\n\nHere, u solves A(p) u = b(p) with A and b having affine parameter dependence, and V is a matrix with columns defining bases for approximation spaces u ≈ V u_r.\n\n\n\n\n\n","category":"type"},{"location":"index.html#ModelOrderReductionToolkit.residual_norm_affine_init","page":"Docstrings","title":"ModelOrderReductionToolkit.residual_norm_affine_init","text":"residual_norm_affine_init(Ais, makeθAi, bis, makeθbi, V[, X0=nothing, T=Float64])\n\nMethod that constructs the necessary vectors and matrices to quickly compute the X-norm of the residual,  r(u_r,p) = A(p) (u - V u_r) = b(p) - A(p) V u_r, by taking advantage of affine parameter dependence of A(p) and b(p).\n\nPass as input a vector of matrices Ais, and a function makeθAi such that the affine construction of A is given by A(p) = ∑_{i=1}^QA makeθAi(p,i) * Ais[i], and similarly a vector of vectors bis and a function makeθbi such that the affine construction of b is given by  b(p) = ∑_{i=1}^Qb makeθbi(p,i) * bis[i].\n\nAdditionally, pass in a matrix V which contains as columns a basis for a reduced space, u ≈ V u_r with the dimension of u_r less than that of u.\n\nOptionally pass in a matrix X from which the X-norm of the residual will be computed in the method residual_norm_affine_online. If X remains as nothing, then will choose it to be the identity matrix to compute the 2-norm of the residual.\n\nIf using complex numbers, specify T=ComplexF64.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.add_col_to_V!","page":"Docstrings","title":"ModelOrderReductionToolkit.add_col_to_V!","text":"add_col_to_V!(res_init, v, T)\n\nMethod to add a vector v to the columns of the matrix V in the Affine_Residual_Init object, res_init, without recomputing all terms. Must specify type T.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.residual_norm_affine_online","page":"Docstrings","title":"ModelOrderReductionToolkit.residual_norm_affine_online","text":"residual_norm_affine_online(res_init, u_r, p)\n\nMethod that given res_init, an Affine_Residual_Init object, computes the X-norm of the residual,  r(u_r,p) = A(p) (u - V u_r) = b(p) - A(p) V u_r, by taking advantage of affine parameter dependence of A(p) and b(p).\n\nPass as input the Affine_Residual_Init object, res_init, a reduced vector u_r, and the corresponding parameter vector p.\n\n\n\n\n\n","category":"function"},{"location":"index.html#ModelOrderReductionToolkit.residual_norm_explicit","page":"Docstrings","title":"ModelOrderReductionToolkit.residual_norm_explicit","text":"residual_norm_explicit(u_approx, p, makeA, makeb, X=nothing)\n\nMethod that computes the X-norm of the residual,  r(u_r,p) = A(p) (u - u_approx) = b(p) - A(p) u_approx explicitly where u is the solution to A(p) u = b(p), and u_approx is an approximation, u ≈ u_approx.\n\nPass as input the approximation vector u_approx, the corresponding parameter, p, the method makeA(p) for constructing the matrix A(p), and the method makeb(p) for constructing the vector b(p).\n\nOptionally pass in a matrix X from which the X-norm of the residual will be computed. If X remains as nothing, then will choose it to be the identity matrix to compute the 2-norm of the residual.\n\n\n\n\n\nresidual_norm_explicit(u_approx, p, Ais, makeθAi, bis, makeθbi, X=nothing)\n\nMethod that computes the X-norm of the residual,  r(u_r,p) = A(p) (u - u_approx) = b(p) - A(p) u_approx explicitly where x is the solution to A(p) u = b(p), and u_approx is an approximation, u ≈ u_approx.\n\nPass as input the approximation vector u_approx, the corresponding parameter, p, a vector of matrices Ais,  and a function makeθAi such that the affine construction  of A is given by A(p) = ∑_{i=1}^QA makeθAi(p,i) * Ais[i],  and similarly a vector of vectors bis and a function makeθbi  such that the affine construction of b is given by  b(p) = ∑_{i=1}^Qb makeθbi(p,i) * bis[i].\n\nOptionally pass in a matrix X from which the X-norm of the residual will be computed. If X remains as nothing, then will choose it to be the identity matrix to compute the 2-norm of the residual.\n\n\n\n\n\n","category":"function"},{"location":"rbm_tutorial.html#Reduced-Basis-Method-Tutorial","page":"RBM Tutorial","title":"Reduced Basis Method Tutorial","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"This tutorial follows closely to the book Reduced Basis Methods for Partial Differential Equations by Quateroni, Alfie, Manzoni, and Negri. For more information, see their text, source 2.","category":"page"},{"location":"rbm_tutorial.html#Problem-formulation-and-motivation","page":"RBM Tutorial","title":"Problem formulation and motivation","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"In this tutorial, we consider scalar, linear, elliptic, parametrized PDEs of the form","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"mathcalL(u(xp)p) = f(xp)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"where p is some parameter (vector), and the solution u depends on a spatial variable x and the parameter. We are interested in such problems specifically as upon discretization, say with finite elements, the discrete problem can be written in the form","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"A(p) u(p) = b(p)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"where A(p)inmathbbR^Ntimes N, u(p)inmathbbR^N, and b(p)inmathbbR^N. Additionally, we will assume affine parameter dependence, i.e., we can write A(p) as ","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"A(p) = sum_i=1^QA theta_i^A(p) A_i","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"and b(p) as","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"b(p) = sum_i=1^Qb theta_i^b(p) b_i","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Note that if a problem does not match this form, there exist algorithms ((D)EIM) to convert the problem to this form.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Upon sufficient discretization, we expect N to be large, and thus the problem of inverting A(p) several times for different parameter values can be expensive. A model order reduction technique is to build a reduced basis (RB) approximation to the solution. To do this, we wish to build an appropriate r dimensional RB space, with r ll N, on wish to use Galerkin projection. ","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Specifically, given linearly independent (assumed orthogonal) basis vectors to this space, v_i_i=1^r, we construct the RB space matrix","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"V = beginbmatrix         v_1  v_2  cdots  v_r           endbmatrix in mathbbR^N times r","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"such that the problem can be approximated by","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"V^T A(p) V u_r(p) = V^T bquad u(p) approx V u_r(p)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"where now the task is to invert the much smaller, rtimes r matrix, V^T A(p) V to form u_r(p), and then the solution is approximated by V u_r(p). Additionally, due to the affine parameter dependence of A(p), we need not store any terms that depend on N, rather we only need to store the matrices V^T A_i V in mathbbR^rtimes r for i=1ldotsQA.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Now, suppose we wished to solve an inverse problem, such as finding the parameter vector p^* that yields some some 'optimal' solution u^*(p). Or suppose that we wish to perform a sensitivity analysis of u(p) on several different parameter values p. These tasks would typically require us to solve the full-order problem a large number of times which may be computationally expensive. ","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"If we are willing to spend offline time to generate an RB space, V, with dimension rll N, then we can much more efficiently spend time online computing the Galerkin projected solution, V u_r(p), at a fraction of a cost of computing the full-order solution.","category":"page"},{"location":"rbm_tutorial.html#Proper-Orthogonal-Decomposition/Principal-Component-Analysis","page":"RBM Tutorial","title":"Proper Orthogonal Decomposition/Principal Component Analysis","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"For now, we will assume a discrete set of parameters mathcalP = p_1ldotsp_P from which we wish to make estimations on. Additionally, we will suppose we know all of the full-order solutions, and store them in the solution matrix","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"S = beginbmatrix         u(p_1)  u(p_2)  cdots  u(p_P)           endbmatrix in mathbbR^N times P","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Note that computing this full matrix may be very expensive if N is large.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We know from the Schmidt-Eckart-Young theorem that the r-dimensional linear subspace that captures the most \"energy\" from the solutions in S (per the Frobenius norm) is the one spanned by the first r left singular vectors of S. More specifically, if we denote VinmathbbR^Ntimes r to be the matrix whose r columns are the first r left singular vectors of S, and let sigma_1geqsigma_2geqldotsgeqsigma_N be the singular values of S, then we can write that","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"S - VV^TS_F = min_textrank(B)leq r A - B_F = sqrtsum_i=r+1^N sigma_i^2","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"where VV^TS is the projection of S onto the columns of V.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"using ModelOrderReductionToolkit\nusing LinearAlgebra\nusing Plots\nusing Colors\nusing SparseArrays\nusing Random\nRandom.seed!(1)\ngr()\n# Boundary conditions\nuleft = 0.0\nuright = 1.0\np_len = 2\nκ_i(i,x) = 1.1 .+ sin.(2π * i * x)\nκ(x,p) = sum([p[i] * κ_i(i,x) for i in 1:p_len])\n# Parameter dependent source term\nf_i(i,x) = i == 1 ? (x .> 0.5) .* 10.0 : 20 .* sin.(2π * (i-1) * x)\nf(x,p) = f_i(1,x) .+ sum([p[i-1] * f_i(i,x) for i in 2:p_len+1])\n# Space setup\nh = 1e-3\nxs = Vector((0:h:1)[2:end-1])\nxhalfs = ((0:h:1)[1:end-1] .+ (0:h:1)[2:end]) ./ 2\nN = length(xs)\n# Helper to generate random parameter vector\nrandP() = 0.1 .+ 2 .* rand(p_len)\n# Ai matrices\nfunction makeAi(i)\n    A = spzeros(N,N)\n    for j in 1:N\n        A[j,j]   = (κ_i(i, xhalfs[j]) + κ_i(i, xhalfs[j+1])) / h^2\n        if j<N\n            A[j,j+1] = -1 * κ_i(i, xhalfs[j+1]) / h^2\n        end\n        if j>1\n            A[j,j-1] = -1 * κ_i(i, xhalfs[j]) / h^2\n        end\n    end\n    return A\nend\nAis = Matrix{Float64}[]\nfor i in 1:p_len\n    push!(Ais, makeAi(i))\nend\n# θAis\nfunction makeθAi(p,i)\n    return p[i]\nend\nfunction makeA(p)\n    A = spzeros(size(Ais[1]))\n    for i in eachindex(Ais)\n        A .+= makeθAi(p,i) .* Ais[i]\n    end\n    return A\nend\n# bi vectors\nfunction makebi(i)\n    b = f_i(i,xs)\n    if i > 1\n            b[1] += uleft * κ_i(i, xhalfs[1]) / h^2\n            b[end] += uright * κ_i(i, xhalfs[end]) / h^2\n        end\n    return b\nend\nbis = Vector{Float64}[]\nfor i in 1:p_len+1\n    push!(bis, makebi(i))\nend\n# θbis\nfunction makeθbi(p,i)\n    if i == 1\n        return 1.0\n    else\n        return p[i-1]\n    end\nend\nfunction makeb(p)\n    b = zeros(size(bis[1]))\n    for i in eachindex(bis)\n        b .+= makeθbi(p,i) .* bis[i]\n    end\n    return b\nend\n\nparams = []\nfor p1 in range(0.1,2.1,10)\n    for p2 in range(0.1,2.1,10)\n        push!(params, [p1,p2])\n    end\nend\nP = length(params)\n\nS = zeros(N, P)\nfor i in 1:P\n    p = params[i]\n    u = makeA(p) \\ makeb(p)\n    S[:,i] .= u\nend","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Suppose that we have the following solution set with N=999 and P=100 (see test problem):","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"S","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\nfor i in 1:P\n    plot!(xs, S[:,i],label=false,alpha=0.25)\nend\ntitle!(\"Solution Set\")\nsavefig(plt, \"rbm_tut1.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We can explicitly compute the SVD and pull the first r columns as ","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"r = 4\nU,s,_ = svd(S)\nV = U[:,1:r]\nnothing; # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We can also plot the exponential singular value decay, suggesting to us that such a RBM will perform well.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot(s, yaxis=:log, label=false)\nyaxis!(\"Singular Values\")\nxaxis!(\"Dimension\")\nsavefig(plt, \"rbm_tut2.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Now, the Schmidt-Eckart-Young theorem tells us that this basis is optimal in the sense that it minimizes l^2 error in directly projecting our solutions, i.e., performing u(p) approx VV^Tu(p). Let's visualize the accuracy of these projections.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\ncolors = palette(:tab10)\nidxs = [rand(1:P) for i in 1:6]\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(xs, S[:,idx], c=colors[i], label=false)\n    u_approx = V * V' * S[:,idx]\n    plot!(xs, u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected POD solutions\")\nsavefig(plt, \"rbm_tut3.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"However, we wished to create a reduced order model (ROM) such that given any new parameter value, we can quickly reproduce a new solution. As was noted before, we do this through a Galerkin projection","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"V^T A(p) V u_r(p) = V^T b implies u(p) approx V u_r(p)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"from which we require only inverting a 4times4 matrix. Although this is no longer guaranteed \"optimal\" by the Schmidt-Eckart-Young theorem, let's see how this performs on the same snapshots.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\ncolors = palette(:tab10)\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(xs, S[:,idx], c=colors[i], label=false)\n    u_r = (V' * makeA(p) * V) \\ (V' * makeb(p))\n    plot!(xs, V * u_r, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected Galerkin POD solutions\")\nsavefig(plt, \"rbm_tut4.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"As we can see from these plots, a 4-dimensional approximation is quite accurate here! Even though after discretization, these solutions lie in mathbbR^999, we have shown that the solution manifold lies approximately on a small dimensional space. Additionally, even though we were only guaranteed \"optimality\" from direct projection of solutions, we still have very good accuracy when we use a Galerkin projection on the problem.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"This process of projection onto left singular values is typically called Proper Orthogonal Decomposition (POD), or Principal Component Analysis (PCA).","category":"page"},{"location":"rbm_tutorial.html#Strong-Greedy-Algorithm","page":"RBM Tutorial","title":"Strong Greedy Algorithm","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"An alternative way to generate this reduced basis is through a process called the strong greedy algorithm. This algorithm is called greedy, because we iteratively choose basis elements in a greedy way. We begin by choosing v_1 to be the column of our solution matrix, S with the largest norm, and then normalized it by its length","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"s_1^* = max_i s_iquad v_1 = fracs_1^*s_1^*","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Now, we use a Gram-Schmidt procedure to orthogonalize all other columns of S with respect to v_1:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"s_i^(1) = s_i - (v_1^T s_i) v_1quad i=1ldotsP","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"After the j-1'st element v_j-1 is chosen and all of the orthogonalization is performed, we then choose v_j to be the column of S^(j-1) which has the largest norm, i.e., has the worst projection error:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"s_j^* = max_i s_i^(j-1)quad v_j = fracs_j^*s_j^*","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"and again orthogonalize","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"s_i^(j) = s_i^(j-1) - (v_j^T s_i^(j-1)) v_jquad i=1ldotsP","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Note that this procedure is exactly like performing a pivoted QR factorization on the matrix S. Let's form this reduced basis of dimension 4:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"r = 4\nQ,_,_ = qr(S, LinearAlgebra.ColumnNorm())\nV = Q[:,1:r]\nnothing; # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Now, we will play the same game. First, we directly project the solutions onto this space","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(xs, S[:,idx], c=colors[i], label=false)\n    u_approx = V * V' * S[:,idx]\n    plot!(xs, u_approx, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected QR solutions\")\nsavefig(plt, \"rbm_tut5.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"And again, we can form a ROM by Galerkin projection:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(xs, S[:,idx], c=colors[i], label=false)\n    u_r = (V' * makeA(p) * V) \\ (V' * makeb(p))\n    plot!(xs, V * u_r, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and projected Galerkin QR solutions\")\nsavefig(plt, \"rbm_tut6.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"This procedure also performs quite well. We may expect the POD/PCA algorithm to be a bit more accurate/general as it can choose basis elements that are not in the columns of S.","category":"page"},{"location":"rbm_tutorial.html#Weak-Greedy-Algorithm","page":"RBM Tutorial","title":"Weak Greedy Algorithm","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Now, one downside of the above procedures was that we needed the matrix of full-order solutions ahead of time to perform either the SVD or QR factorizations. If our model was very computationally expensive, we would not want to have to do this. This is where the weak greedy algorithm is useful. It is again a greedy algorithm as we will be choosing \"columns\" greedily, but we wish to not have to construct all columns directly.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Suppose now, instead of having access to the columns s_i which correspond to the full-order solutions u(p_i), we only have access to the parameter values p_i. One can show that there exists an upper-bound on projection error, given by","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"u(p) - V u_r(p) = A(p)^-1 b(p) - V u_r(p) leq fracb(p) - A(p) V u_r(p)sigma_min(A(p))","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"where sigma_min(A(p)) is the minimum singular value of A(p). Note that this upperbound on the error does not depend on the full order solution, u(p). So, we loop through each parameter vector p_i, and select the one, p^* that yields the highest upper-bound error. We then form the full-order solution u(p_i), normalize it, and append it as a column of V. Note that unlike in the strong algorithm, since we are not using true error, we are not guaranteed to choose the next \"best\" column of V. However, if we are computing a reduced basis of size r, then we only need to call the full-order model r times.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We now need a method to approximate (a lowerbound of ) sigma_min(A(p)), and then the numerator of the above can be computed explicitly. One way of doing this is through the successive constraint method (SCM). This method takes advantage of the affine parameter dependence of A(p), see source 1. To compute a successive constraint object, taking note that our A(p) is a positive definite matrix, we use the following code:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"using ModelOrderReductionToolkit\nMa = 50; Mp = 15; ϵ_SCM = 1e-2;\nscm = initialize_SCM_SPD(params, Ais, makeθAi, Ma, Mp, ϵ_SCM)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We can then create a lower-bound approximation of the singular value of A(p) by calling it directly:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"scm(randP())","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"With this in place, we have enough to construct the weak greedy RB method. Note that the method below requires each of the Ais, bis, and methods makeθAi and makeθbi for the affine construction of A(p) and b(p). This is so that looping over the parameter set does not depend on the large dimension N.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"ϵ_greedy = 1e-1  \nr = 4\ngreedy_sol = GreedyRBAffineLinear(params, Ais, makeθAi, bis, makeθbi, scm, ϵ_greedy, max_snapshots=r)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We can access the greedily chosen reduced basis by calling (note that for computational purposes, V is stored as a vector of vectors instead of a matrix)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"V = reduce(hcat, greedy_sol.V)","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We can now visualize these solutions by calling greedy_sol(p) on a paramater vector p.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot()\nfor i in 1:6\n    idx = idxs[i]\n    p = params[idx]\n    plot!(xs, S[:,idx], c=colors[i], label=false)\n    u_r = greedy_sol(p,false) # full=false, size r vector instead of N\n    plot!(xs, V * u_r, c=colors[i], label=false, ls=:dash)\nend\ntitle!(\"Truth and weak greedy solutions\")\nsavefig(plt, \"rbm_tut7.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html#Comparison-of-the-methods","page":"RBM Tutorial","title":"Comparison of the methods","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"The following method produces all full-order solutions, and then computes errors associated with the weak greedy, strong greedy, and POD/PCA algorithms.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"data = greedy_rb_err_data(params, Ais, makeθAi, bis, makeθbi, scm, 50, noise=0)\nnothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"First, we wish to plot the error decay of projecting solutions onto the POD/PCA space, and of forming a Galerkin procedure for POD/PCA:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plt = plot(data[:basis_dim], data[:pca_proj], label=\"pod/pca proj\", yaxis=:log, xlabel=\"r\", ylabel=\"error\")\nplot!(data[:basis_dim], data[:pca_err], label=\"pod/pca Galerkin\")\ntitle!(\"Error estimates for RBMs\")\nsavefig(plt, \"rbm_tut8.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"Here, we see that we have exponential convergence in both cases, but the direct projection method has slightly better accuracy. We can add in the plots for the strong greedy method.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plot!(data[:basis_dim], data[:strong_greedy_proj], label=\"sg proj\")\nplot!(data[:basis_dim], data[:strong_greedy_err], label=\"sg Galerkin\")\nsavefig(plt, \"rbm_tut9.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"We have the same exponential convergence rates. Finally, we can add in the convergence rates for the weak greedy upperbound error and truth errors.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"plot!(data[:basis_dim], data[:weak_greedy_ub], label=\"wg ub\")\nplot!(data[:basis_dim], data[:weak_greedy_true], label=\"wg truth\")\nplot!(data[:basis_dim], data[:weak_greedy_true_ub], label=\"wg truth ub\")\nsavefig(plt, \"rbm_tut10.svg\"); nothing # hide","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"(Image: )","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"A few things to note with these curves. ","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"The wg ub curve tells us the maximal upperbound approximation of error used to choose the next snapshot. This is of higher magnitude due to the error we make in approximating the error upperbound.\nThe wg truth curve tells us the truth error of the snapshot chosen with the highest upper-bound error. This curve oscillates as we do not pick the \"optimal\" strong-greedy snapshot each time.\nThe wg truth ub curve tells us the maximal truth error across snapshots, similar to what would be used in the strong greedy algorithm. This curve is strictly above wg truth, and is equal when we do in fact choose the optimal snapshot.\nAlthough wg truth ub is above sg Galerkin, telling us that we are not making optimal greedy choices, it decays at the same exponential rate telling us that the weak greedy algorithm is still making good choices.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"In conclusion, the weak greedy algorithm takes advantage of the affine parameter dependence of A(p) and b(p), and uses an upperbound error approximator to produce a reduced basis that approximates solutions nearly as well as the strong greedy algorithm and the POD/PCA algorithm.","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"In order to replicate the code in this tutorial, you will need to run the following setup code which is hidden from the start of this tutorial:","category":"page"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"using ModelOrderReductionToolkit\nusing LinearAlgebra\nusing Plots\nusing Colors\nusing SparseArrays\nusing Random\nRandom.seed!(1)\ngr()\n# Boundary conditions\nuleft = 0.0\nuright = 1.0\np_len = 2\nκ_i(i,x) = 1.1 .+ sin.(2π * i * x)\nκ(x,p) = sum([p[i] * κ_i(i,x) for i in 1:p_len]) \n# Parameter dependent source term\nf_i(i,x) = i == 1 ? (x .> 0.5) .* 10.0 : 20 .* sin.(2π * (i-1) * x)\nf(x,p) = f_i(1,x) .+ sum([p[i-1] * f_i(i,x) for i in 2:p_len+1])\n# Space setup\nh = 1e-3\nxs = Vector((0:h:1)[2:end-1])\nxhalfs = ((0:h:1)[1:end-1] .+ (0:h:1)[2:end]) ./ 2\nN = length(xs)\n# Helper to generate random parameter vector\nrandP() = 0.1 .+ 2 .* rand(p_len)\n# Ai matrices\nfunction makeAi(i)\n    A = spzeros(N,N)\n    for j in 1:N\n        A[j,j]   = (κ_i(i, xhalfs[j]) + κ_i(i, xhalfs[j+1])) / h^2\n        if j<N\n            A[j,j+1] = -1 * κ_i(i, xhalfs[j+1]) / h^2\n        end\n        if j>1\n            A[j,j-1] = -1 * κ_i(i, xhalfs[j]) / h^2\n        end\n    end\n    return A\nend\nAis = Matrix{Float64}[]\nfor i in 1:p_len\n    push!(Ais, makeAi(i))\nend\n# θAis\nfunction makeθAi(p,i)\n    return p[i]\nend\nfunction makeA(p)\n    A = zeros(size(Ais[1]))\n    for i in eachindex(Ais)\n        A .+= makeθAi(p,i) .* Ais[i]\n    end\n    return A\nend\n# bi vectors\nfunction makebi(i)\n    b = f_i(i,xs)\n    if i > 1\n            b[1] += uleft * κ_i(i, xhalfs[1]) / h^2\n            b[end] += uright * κ_i(i, xhalfs[end]) / h^2\n        end\n    return b\nend\nbis = Vector{Float64}[]\nfor i in 1:p_len+1\n    push!(bis, makebi(i))\nend\n# θbis\nfunction makeθbi(p,i)\n    if i == 1\n        return 1.0\n    else\n        return p[i-1]\n    end\nend\nfunction makeb(p)\n    b = zeros(size(bis[1]))\n    for i in eachindex(bis)\n        b .+= makeθbi(p,i) .* bis[i]\n    end\n    return b\nend\n\nparams = []\nfor p1 in range(0.1,2.1,10)\n    for p2 in range(0.1,2.1,10)\n        push!(params, [p1,p2])\n    end\nend\nP = length(params)\n\nS = zeros(N, P)\nfor i in 1:P\n    p = params[i]\n    u = makeA(p) \\ makeb(p)\n    S[:,i] .= u\nend","category":"page"},{"location":"rbm_tutorial.html#References:","page":"RBM Tutorial","title":"References:","text":"","category":"section"},{"location":"rbm_tutorial.html","page":"RBM Tutorial","title":"RBM Tutorial","text":"D.B.P. Huynh, G. Rozza, S. Sen, A.T. Patera. A successive constraint linear optimization method for lower bounds of parametric coercivity and inf–sup stability constants. Comptes Rendus Mathematique. Volume 345, Issue 8. 2007. Pages 473-478. https://doi.org/10.1016/j.crma.2007.09.019.\nQuarteroni, Alfio, Andrea Manzoni, and Federico Negri. Reduced Basis Methods for Partial Differential Equations. Vol. 92. UNITEXT. Cham: Springer International Publishing, 2016. http://link.springer.com/10.1007/978-3-319-15431-2.","category":"page"}]
}
