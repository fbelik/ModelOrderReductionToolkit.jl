<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RBM Tutorial · ModelOrderReductionToolkit.jl</title><meta name="title" content="RBM Tutorial · ModelOrderReductionToolkit.jl"/><meta property="og:title" content="RBM Tutorial · ModelOrderReductionToolkit.jl"/><meta property="twitter:title" content="RBM Tutorial · ModelOrderReductionToolkit.jl"/><meta name="description" content="Documentation for ModelOrderReductionToolkit.jl."/><meta property="og:description" content="Documentation for ModelOrderReductionToolkit.jl."/><meta property="twitter:description" content="Documentation for ModelOrderReductionToolkit.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">ModelOrderReductionToolkit.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Models and Reductors</a></li><li class="is-active"><a class="tocitem" href="rbm_tutorial.html">RBM Tutorial</a></li><li><a class="tocitem" href="docs.html">Additional Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="rbm_tutorial.html">RBM Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="rbm_tutorial.html">RBM Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/fbelik/ModelOrderReductionToolkit.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/fbelik/ModelOrderReductionToolkit.jl/blob/master/docs/src/rbm_tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reduced-Basis-Method-Tutorial"><a class="docs-heading-anchor" href="#Reduced-Basis-Method-Tutorial">Reduced Basis Method Tutorial</a><a id="Reduced-Basis-Method-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Reduced-Basis-Method-Tutorial" title="Permalink"></a></h1><p>This tutorial follows closely to the book Reduced Basis Methods for Partial Differential Equations by Quateroni, Alfie, Manzoni, and Negri. For more information, see their text, source 2.</p><h3 id="Problem-formulation-and-motivation"><a class="docs-heading-anchor" href="#Problem-formulation-and-motivation">Problem formulation and motivation</a><a id="Problem-formulation-and-motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-formulation-and-motivation" title="Permalink"></a></h3><p>In this tutorial, we consider scalar, linear, elliptic, parametrized PDEs of the form</p><p class="math-container">\[\mathcal{L}(u(x,p),p) = f(x,p)\]</p><p>where <span>$p$</span> is some parameter (vector), and the solution <span>$u$</span> depends on a spatial variable <span>$x$</span> and the parameter. We are interested in such problems specifically as upon discretization, say with finite elements, the discrete problem can be written in the form</p><p class="math-container">\[A(p) u(p) = b(p)\]</p><p>where <span>$A(p)\in\mathbb{R}^{N\times N}$</span>, <span>$u(p)\in\mathbb{R}^N$</span>, and <span>$b(p)\in\mathbb{R}^N$</span>. Additionally, we will assume <strong>affine</strong> parameter dependence, i.e., we can write <span>$A(p)$</span> as </p><p class="math-container">\[A(p) = \sum_{i=1}^{QA} \theta_i^A(p) A_i,\]</p><p>and <span>$b(p)$</span> as</p><p class="math-container">\[b(p) = \sum_{i=1}^{Qb} \theta_i^b(p) b_i.\]</p><p>Note that if a problem does not match this form, there exist algorithms ((D)EIM) to convert the problem to this form.</p><p>Upon sufficient discretization, we expect <span>$N$</span> to be large, and thus the problem of inverting <span>$A(p)$</span> several times for different parameter values can be expensive. A model order reduction technique is to build a reduced basis (RB) approximation to the solution. To do this, we wish to build an appropriate <span>$r$</span> dimensional RB space, with <span>$r \ll N$</span>, on wish to use Galerkin projection. </p><p>Specifically, given linearly independent (assumed orthogonal) basis vectors to this space, <span>$\{v_i\}_{i=1}^r$</span>, we construct the RB space matrix</p><p class="math-container">\[V = \begin{bmatrix} | &amp; | &amp;  &amp; | \\ v_1 &amp; v_2 &amp; \cdots &amp; v_r \\  | &amp; | &amp;  &amp; |  \end{bmatrix} \in \mathbb{R}^{N \times r}\]</p><p>such that the problem can be approximated by</p><p class="math-container">\[V^T A(p) V u_r(p) = V^T b,\quad u(p) \approx V u_r(p).\]</p><p>where now the task is to invert the much smaller, <span>$r\times r$</span> matrix, <span>$V^T A(p) V$</span> to form <span>$u_r(p)$</span>, and then the solution is approximated by <span>$V u_r(p)$</span>. Additionally, due to the affine parameter dependence of <span>$A(p)$</span>, we need not store any terms that depend on <span>$N$</span>, rather we only need to store the matrices <span>$V^T A_i V \in \mathbb{R}^{r\times r}$</span> for <span>$i=1,\ldots,QA$</span>.</p><p>Now, suppose we wished to solve an inverse problem, such as finding the parameter vector <span>$p^*$</span> that yields some some &#39;optimal&#39; solution <span>$u^*(p)$</span>. Or suppose that we wish to perform a sensitivity analysis of <span>$u(p)$</span> on several different parameter values <span>$p$</span>. These tasks would typically require us to solve the full-order problem a large number of times which may be computationally expensive. </p><p>If we are willing to spend <strong>offline</strong> time to generate an RB space, <span>$V$</span>, with dimension <span>$r\ll N$</span>, then we can much more efficiently spend time <strong>online</strong> computing the Galerkin projected solution, <span>$V u_r(p)$</span>, at a fraction of a cost of computing the full-order solution.</p><p>We will consider a steady state heat equation for this tutorial with affine-parameter dependent spacial diffusion coefficient and forcing terms. Once discretized, it can be written in the form <span>$A(p) u = b(p)$</span> with <span>$A(p)$</span> and <span>$b(p)$</span> each with affine parameter dependence. This model can be instantiated in <code>ModelOrderReductionToolkit.jl</code> by calling <code>PoissonModel()</code>.</p><pre><code class="language-julia hljs">using ModelOrderReductionToolkit
model = PoissonModel()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A(p) x(p) = b(p) with output length 999
A - (999, 999) affine parameter dependent array with 3 terms
b - (999,) affine parameter dependent array with 4 terms</code></pre><p>We can then form a snapshot matrix over a set of <span>$P=125$</span> parameter vectors.</p><pre><code class="language-julia hljs">params = [[i,j,k] for i in range(0,1,5) for j in range(0,1,5) for k in range(0,1,5)]
P = length(params)

S = zeros(output_length(model), P)
for i in 1:P
    p = params[i]
    u = model(p)
    S[:,i] .= u
end

S</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">999×125 Matrix{Float64}:
 0.00311875  0.00313681  0.00315495  …  0.00439843  0.0045139   0.00469196
 0.0062375   0.00626902  0.00630066     0.00875012  0.00897341  0.00932075
 0.00935625  0.00939663  0.00943717     0.0130561   0.0133798   0.0138881
 0.012475    0.0125197   0.0125646      0.0173173   0.0177345   0.0183955
 0.0155938   0.0156382   0.0156829      0.0215346   0.0220386   0.0228448
 0.0187125   0.0187521   0.0187922   …  0.0257091   0.0262933   0.0272374
 0.0218313   0.0218615   0.0218926      0.0298417   0.0304999   0.0315747
 0.02495     0.0249665   0.0249841      0.0339331   0.0346595   0.0358584
 0.0280688   0.028067    0.0280669      0.0379844   0.0387732   0.0400897
 0.0311875   0.031163    0.0311409      0.0419963   0.042842    0.04427
 ⋮                                   ⋱                          
 0.0280688   0.0280714   0.0280739   …  1.01719     1.01617     1.01454
 0.02495     0.0249339   0.0249174      1.01519     1.01428     1.01282
 0.0218313   0.021801    0.0217702      1.01321     1.01241     1.01114
 0.0187125   0.0186728   0.0186323      1.01126     1.01057     1.00948
 0.0155938   0.0155492   0.0155039      1.00933     1.00875     1.00784
 0.012475    0.0124301   0.0123847   …  1.00742     1.00696     1.00623
 0.00935625  0.00931572  0.00927472     1.00554     1.00518     1.00464
 0.0062375   0.0062059   0.006174       1.00367     1.00343     1.00307
 0.00311875  0.00310066  0.00308244     1.00183     1.00171     1.00152</code></pre><p>Let&#39;s visualize the solutions.</p><pre><code class="language-julia hljs">using Plots
plt = plot()
for i in 1:P
    plot!(S[:,i],label=false,alpha=0.25)
end
title!(&quot;Solution Set&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GKS: cannot open display - headless operation mode active</code></pre><p><img src="rbm_tut1.svg" alt/></p><h3 id="Proper-Orthogonal-Decomposition/Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Proper-Orthogonal-Decomposition/Principal-Component-Analysis">Proper Orthogonal Decomposition/Principal Component Analysis</a><a id="Proper-Orthogonal-Decomposition/Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Proper-Orthogonal-Decomposition/Principal-Component-Analysis" title="Permalink"></a></h3><p>We know from the Schmidt-Eckart-Young theorem that the <span>$r$</span>-dimensional linear subspace that captures the most &quot;energy&quot; from the solutions in <span>$S$</span> (per the Frobenius norm) is the one spanned by the first <span>$r$</span> left singular vectors of <span>$S$</span>. More specifically, if we denote <span>$V\in\mathbb{R}^{N\times r}$</span> to be the matrix whose <span>$r$</span> columns are the first <span>$r$</span> left singular vectors of <span>$S$</span>, and let <span>$\sigma_1\geq\sigma_2\geq\ldots\geq\sigma_N$</span> be the singular values of <span>$S$</span>, then we can write that</p><p class="math-container">\[||S - VV^TS||_F = \min_{\text{rank}(B)\leq r} ||A - B||_F = \sqrt{\sum_{i=r+1}^N \sigma_i^2}.\]</p><p>where <span>$VV^TS$</span> is the projection of <span>$S$</span> onto the columns of <span>$V$</span>.</p><p>We can explicitly compute the SVD and pull the first <span>$r$</span> columns as </p><pre><code class="language-julia hljs">using LinearAlgebra
r = 5
U,s,_ = svd(S)
V = U[:,1:r]</code></pre><p>We can also plot the exponential singular value decay, suggesting to us that such an RBM will perform well.</p><pre><code class="language-julia hljs">plt = plot(s, yaxis=:log, label=false)
yaxis!(&quot;Singular Values&quot;)
xaxis!(&quot;Dimension&quot;)</code></pre><p><img src="rbm_tut2.svg" alt/></p><p>Now, the Schmidt-Eckart-Young theorem tells us that this basis is optimal in the sense that it minimizes <span>$l^2$</span> error in directly projecting our solutions, i.e., performing <span>$u(p) \approx VV^Tu(p)$</span>. Let&#39;s visualize the accuracy of these projections.</p><pre><code class="language-julia hljs">plt = plot()
colors = palette(:tab10)
idxs = [rand(1:P) for i in 1:6]
for i in 1:6
    idx = idxs[i]
    p = params[idx]
    plot!(S[:,idx], c=colors[i], label=false)
    u_approx = V * V&#39; * S[:,idx]
    plot!(u_approx, c=colors[i], label=false, ls=:dash)
end
title!(&quot;Truth and projected POD solutions&quot;)</code></pre><p><img src="rbm_tut3.svg" alt/></p><p>However, we wished to create a reduced order model (ROM) such that given any new parameter value, we can quickly reproduce a new solution. As was noted before, we do this through a Galerkin projection</p><p class="math-container">\[V^T A(p) V u_r(p) = V^T b \implies u(p) \approx V u_r(p) = u_\text{approx}(p)\]</p><p>from which we require only inverting an <span>$r\times r$</span> matrix. Although this is no longer guaranteed &quot;optimal&quot; by the Schmidt-Eckart-Young theorem, let&#39;s see how this performs on the same snapshots. To perform this task in <code>ModelOrderReductionToolkit.jl</code>, we pass the snapshot matrix into a <code>PODReductor</code> object and form a ROM from the reductor.</p><pre><code class="language-julia hljs">pod_reductor = PODReductor(model)
add_to_rb!(pod_reductor, S)
pod_rom = form_rom(pod_reductor, r)
plt = plot()
colors = palette(:tab10)
for i in 1:6
    idx = idxs[i]
    p = params[idx]
    plot!(S[:,idx], c=colors[i], label=false)
    u_r = pod_rom(p)
    u_approx = lift(pod_reductor, u_r)
    plot!(u_approx, c=colors[i], label=false, ls=:dash)
end
title!(&quot;Truth and projected Galerkin POD solutions&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Forming SVD of snapshot matrix</code></pre><p><img src="rbm_tut4.svg" alt/></p><p>As we can see from these plots, a <span>$5$</span>-dimensional approximation is quite accurate here! Even though after discretization, these solutions lie in <span>$\mathbb{R}^{999}$</span>, we have shown that the solution manifold lies approximately on a <span>$5$</span>-dimensional space. Additionally, even though we were only guaranteed &quot;optimality&quot; from direct projection of solutions, we still have very good accuracy when we use a Galerkin projection on the problem.</p><p>This process of projection onto left singular values is typically called <strong>Proper Orthogonal Decomposition</strong> (POD). Note that forming a <code>PODReductor</code> will call <code>svd</code> on the snapshot matrix. We can access the singular values from <code>pod_reductor.S</code>, and the left-singular vectors from <code>pod_reductor.V</code> (note that left-singular vectors are usually denoted with <span>$U$</span>, but we use <span>$V$</span> to stick with RB notation).</p><h3 id="Strong-Greedy-Algorithm"><a class="docs-heading-anchor" href="#Strong-Greedy-Algorithm">Strong Greedy Algorithm</a><a id="Strong-Greedy-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Strong-Greedy-Algorithm" title="Permalink"></a></h3><p>An alternative way to generate this reduced basis is through a process called the <strong>strong greedy algorithm</strong>. This algorithm is called greedy, because we iteratively choose basis elements in a greedy way. We begin by choosing <span>$v_1$</span> to be the column of our solution matrix, <span>$S$</span> with the largest norm, and then normalized it by its length</p><p class="math-container">\[||s_1^*|| = \max_i ||s_i||,\quad v_1 = \frac{s_1^*}{||s_1^*||}.\]</p><p>Now, we use a Gram-Schmidt procedure to orthogonalize all other columns of <span>$S$</span> with respect to <span>$v_1$</span>:</p><p class="math-container">\[s_i^{(1)} = s_i - (v_1^T s_i) v_1,\quad i=1,\ldots,P.\]</p><p>After the <span>$j-1$</span>&#39;st element <span>$v_{j-1}$</span> is chosen and all of the orthogonalization is performed, we then choose <span>$v_{j}$</span> to be the column of <span>$S^{(j-1)}$</span> which has the largest norm, i.e., has the worst projection error:</p><p class="math-container">\[||s_j^*|| = \max_i ||s_i^{(j-1)}||,\quad v_{j} = \frac{s_j^*}{||s_j^*||},\]</p><p>and again orthogonalize</p><p class="math-container">\[s_i^{(j)} = s_i^{(j-1)} - (v_j^T s_i^{(j-1)}) v_j,\quad i=1,\ldots,P.\]</p><p>Note that this procedure is exactly like performing a pivoted QR factorization on the matrix <span>$S$</span>. Let&#39;s form this reduced basis of the same dimension:</p><pre><code class="language-julia hljs">Q,_,_ = qr(S, LinearAlgebra.ColumnNorm())
V = Q[:,1:r]</code></pre><p>Now, we will play the same game. First, we directly project the solutions onto this space</p><pre><code class="language-julia hljs">plt = plot()
for i in 1:6
    idx = idxs[i]
    p = params[idx]
    plot!(S[:,idx], c=colors[i], label=false)
    u_approx = V * V&#39; * S[:,idx]
    plot!(u_approx, c=colors[i], label=false, ls=:dash)
end
title!(&quot;Truth and projected SG solutions&quot;)</code></pre><p><img src="rbm_tut5.svg" alt/></p><p>Now, we will use an <code>SGReductor</code> object to form a Galerkin-projected reduced order model.</p><pre><code class="language-julia hljs">sg_reductor = SGReductor(model)
add_to_rb!(sg_reductor, S)
sg_rom = form_rom(sg_reductor, r)
plt = plot()
for i in 1:6
    idx = idxs[i]
    p = params[idx]
    plot!(S[:,idx], c=colors[i], label=false)
    u_r = sg_rom(p)
    u_approx = lift(sg_reductor, u_r)
    plot!(u_approx, c=colors[i], label=false, ls=:dash)
end
title!(&quot;Truth and projected Galerkin SG solutions&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Forming column-pivoted QR of snapshot matrix</code></pre><p><img src="rbm_tut6.svg" alt/></p><p>This procedure also performs quite well. We may expect the POD algorithm to be a bit more accurate/general as it can choose basis elements that are not &quot;in the columns&quot; of <span>$S$</span>. Similar to the <code>PODReductor</code> object, we can access the reduced basis from <code>sg_reductor.V</code>.</p><h3 id="Weak-Greedy-Algorithm"><a class="docs-heading-anchor" href="#Weak-Greedy-Algorithm">Weak Greedy Algorithm</a><a id="Weak-Greedy-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Weak-Greedy-Algorithm" title="Permalink"></a></h3><p>Now, one downside of the above procedures was that we needed the matrix of full-order solutions ahead of time to perform either the SVD or QR factorizations. If our model was very computationally expensive, we would not want to have to do this. This is where the <strong>weak greedy algorithm</strong> is useful. It is again a greedy algorithm as we will be choosing &quot;columns&quot; greedily, but we wish to not have to construct all columns directly.</p><p>Suppose now, instead of having access to the columns <span>$s_i$</span> which correspond to the full-order solutions <span>$u(p_i)$</span>, we only have access to the parameter values <span>$p_i$</span>. Generally, we would wish to have an a priori error estimator for a Galerkin-projected ROM such that we could loop over our parameter vectors and choose the one with the estimated maximum error. In this tutorial, we use a stability-residual approach</p><p>One can show that there exists an upper-bound on projection error, given by</p><p class="math-container">\[||u(p) - V u_r(p)|| = ||A(p)^{-1} b(p) - V u_r(p)|| \leq \frac{||b(p) - A(p) V u_r(p)||}{\sigma_{min}(A(p))}\]</p><p>where <span>$\sigma_{min}(A(p))$</span> is the minimum singular value of <span>$A(p)$</span>. Note that this upperbound on the error does not depend on the full order solution, <span>$u(p)$</span>. So, we loop through each parameter vector <span>$p_i$</span>, and select the one, <span>$p^*$</span> that yields the highest upper-bound error. We then form the full-order solution <span>$u(p_i)$</span>, normalize it, and append it as a column of <span>$V$</span>. Note that unlike in the strong algorithm, since we are not using true error, we are not guaranteed to choose the next &quot;best&quot; column of <span>$V$</span>. However, if we are computing a reduced basis of size <span>$r$</span>, then we only need to call the full-order model <span>$r$</span> times.</p><p>We now need a method to approximate (a lowerbound of ) <span>$\sigma_{min}(A(p))$</span>, and then the numerator of the above can be computed explicitly. One way of doing this is through the <strong>successive constraint method</strong> (SCM). This method takes advantage of the affine parameter dependence of <span>$A(p)$</span>, see source 1. We will form an SCM object and initialize an object to compute the norm of the residual through a <code>StabilityResidualErrorEstimator</code>.</p><pre><code class="language-julia hljs">error_estimator = StabilityResidualErrorEstimator(model, params);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Stability Residual Error Estimator
Stability method - SCM Object for matrix (999, 999) affine parameter dependent array with 3 terms
</code></pre><p>Note that this method assumes that the model is coercive, i.e., the matrix <code>A(p)</code> is symmetric positive definite for each parameter. For this model, we know that this is the case if the parameter vectors have entries between 0 and 1. For a noncoercive model, add the keyword argument <code>coercive=false</code>. With this in place, we have enough to construct the weak greedy reductor.</p><pre><code class="language-julia hljs">wg_reductor = WGReductor(model, error_estimator)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">WG reductor with RB dimension 0
FOM: A(p) x(p) = b(p) with output length 999
A - (999, 999) affine parameter dependent array with 3 terms
b - (999,) affine parameter dependent array with 4 terms
ROM: A(p) x(p) = b(p) with output length 0
A - (0, 0) affine parameter dependent array with 3 terms
b - (0,) affine parameter dependent array with 4 terms
Increase RB dimension with add_to_rb!(reductor, params) or add_to_rb!(reductor, params, r)</code></pre><p>Note that we have to build the reduced basis by looping over a parameter set, we will do this by calling <code>add_to_rb!</code>. Afterwards, since the reductor must store the ROM at each step to make error approximations, we can simply pull it from the reductor object.</p><pre><code class="language-julia hljs">add_to_rb!(wg_reductor, params, r)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(1) truth error = 5.4311e+01, upperbound error = 1.8032e+05
(2) truth error = 4.2307e+01, upperbound error = 2.2520e+03
(3) truth error = 1.1490e+01, upperbound error = 3.1515e+02
(4) truth error = 3.2129e+00, upperbound error = 7.3086e+01
(5) truth error = 1.4974e+00, upperbound error = 6.3037e+01</code></pre><pre><code class="language-julia hljs">wg_rom = wg_reductor.rom</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A(p) x(p) = b(p) with output length 5
A - (5, 5) affine parameter dependent array with 3 terms
b - (5,) affine parameter dependent array with 4 terms</code></pre><p>We can access the greedily chosen reduced basis by calling (note that for computational purposes, <span>$V$</span> is stored as a <code>VectorOfVectors</code> object).</p><pre><code class="language-julia hljs">wg_reductor.V</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">999×5 ModelOrderReductionToolkit.VectorOfVectors{Float64}:
 8.63903e-5    5.62701e-5   0.000114226  4.39796e-5    4.5034e-5
 0.000171617   0.000111783  0.000233579  8.96081e-5    0.000104083
 0.000255713   0.000166558  0.000357921  0.000136685   0.000176661
 0.000338706   0.000220615  0.000487118  0.000185019   0.000262299
 0.000420627   0.000273975  0.00062104   0.00023443    0.000360547
 0.000501505   0.000326654  0.000759562  0.000284745   0.000470971
 0.000581367   0.000378672  0.000902562  0.000335803   0.000593152
 0.000660239   0.000430045  0.00104992   0.000387448   0.000726687
 0.000738147   0.00048079   0.00120153   0.000439534   0.000871186
 0.000815118   0.000530925  0.00135727   0.000491923   0.00102627
 ⋮                                                    
 0.0186801    -0.103024     0.0281005    0.0150432    -0.0069394
 0.0186485    -0.103168     0.0278585    0.0147802    -0.00685647
 0.0186175    -0.10331      0.0276127    0.0145194    -0.00676842
 0.0185869    -0.10345      0.0273631    0.014261     -0.00667529
 0.0185567    -0.103588     0.0271099    0.0140051    -0.0065771
 0.018527     -0.103724     0.0268531    0.0137518    -0.00647388
 0.0184978    -0.103858     0.0265929    0.0135011    -0.00636568
 0.0184689    -0.103991     0.0263294    0.0132532    -0.00625252
 0.0184405    -0.104121     0.0260627    0.013008     -0.00613445</code></pre><p>We can now visualize these solutions by calling <code>wg_rom(p)</code> on a paramater vector <code>p</code>.</p><pre><code class="language-julia hljs">plt = plot()
for i in 1:6
    idx = idxs[i]
    p = params[idx]
    plot!(S[:,idx], c=colors[i], label=false)
    u_r = wg_rom(p)
    u_approx = lift(wg_reductor, u_r)
    plot!(u_approx, c=colors[i], label=false, ls=:dash)
end
title!(&quot;Truth and WG solutions&quot;)</code></pre><p><img src="rbm_tut7.svg" alt/></p><h3 id="Comparison-of-the-methods"><a class="docs-heading-anchor" href="#Comparison-of-the-methods">Comparison of the methods</a><a id="Comparison-of-the-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-of-the-methods" title="Permalink"></a></h3><p>Let&#39;s compare the above algorithms by comparing their average and worst case accuracy over the parameter set.</p><pre><code class="language-julia hljs">errors = [Float64[] for _ in 1:3]
for (i,p) in enumerate(params)
    pod_error = norm(lift(pod_reductor, pod_rom(p)) .- S[:,i])
    push!(errors[1], pod_error)
    sg_error = norm(lift(sg_reductor, sg_rom(p)) .- S[:,i])
    push!(errors[2], sg_error)
    wg_error = norm(lift(wg_reductor, wg_rom(p)) .- S[:,i])
    push!(errors[3], wg_error)
end
println(&quot;Errors for RB dimension r=$r&quot;)
println(&quot;POD mean error: $(sum(errors[1]) / length(errors[1]))&quot;)
println(&quot;POD worst error: $(maximum(errors[1]))&quot;)
println(&quot;SG mean error: $(sum(errors[2]) / length(errors[2]))&quot;)
println(&quot;SG worst error: $(maximum(errors[2]))&quot;)
println(&quot;WG mean error: $(sum(errors[3]) / length(errors[3]))&quot;)
println(&quot;WG worst error: $(maximum(errors[3]))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Errors for RB dimension r=5
POD mean error: 0.16333953592206588
POD worst error: 0.4341530046750684
SG mean error: 0.4434598097105646
SG worst error: 0.9329683303719419
WG mean error: 0.40967075796855373
WG worst error: 1.004541382449191</code></pre><p>Let&#39;s repeat the process for a reduced basis of dimension <span>$r=15$</span> Let&#39;s compare the above algorithms by comparing their average and worst case accuracy over the parameter set.</p><pre><code class="language-julia hljs">oldr = r
r = 15
pod_rom = form_rom(pod_reductor, r)
sg_rom = form_rom(sg_reductor, r)
add_to_rb!(wg_reductor, params, r - oldr)
wg_rom = wg_reductor.rom
errors = [Float64[] for _ in 1:3]
for (i,p) in enumerate(params)
    pod_error = norm(lift(pod_reductor, pod_rom(p)) .- S[:,i])
    push!(errors[1], pod_error)
    sg_error = norm(lift(sg_reductor, sg_rom(p)) .- S[:,i])
    push!(errors[2], sg_error)
    wg_error = norm(lift(wg_reductor, wg_rom(p)) .- S[:,i])
    push!(errors[3], wg_error)
end
println(&quot;Errors for RB dimension r=$r&quot;)
println(&quot;POD mean error: $(sum(errors[1]) / length(errors[1]))&quot;)
println(&quot;POD worst error: $(maximum(errors[1]))&quot;)
println(&quot;SG mean error: $(sum(errors[2]) / length(errors[2]))&quot;)
println(&quot;SG worst error: $(maximum(errors[2]))&quot;)
println(&quot;WG mean error: $(sum(errors[3]) / length(errors[3]))&quot;)
println(&quot;WG worst error: $(maximum(errors[3]))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(6) truth error = 9.6385e-01, upperbound error = 4.5721e+01
(7) truth error = 4.1616e-01, upperbound error = 2.2715e+01
(8) truth error = 5.2367e-01, upperbound error = 1.8645e+01
(9) truth error = 1.5078e-01, upperbound error = 1.4025e+01
(10) truth error = 8.3097e-02, upperbound error = 1.4242e+01
(11) truth error = 1.7457e-01, upperbound error = 1.0368e+01
(12) truth error = 5.4074e-02, upperbound error = 7.8470e+00
(13) truth error = 1.7132e-02, upperbound error = 4.4757e+00
(14) truth error = 2.1617e-02, upperbound error = 4.7548e+00
(15) truth error = 1.8859e-02, upperbound error = 3.3648e+00
Errors for RB dimension r=15
POD mean error: 0.0028622298638518204
POD worst error: 0.008770724351391338
SG mean error: 0.004122120213871156
SG worst error: 0.011261695405919285
WG mean error: 0.003900508300378061
WG worst error: 0.012672536651311806</code></pre><p>In conclusion, the weak greedy algorithm takes advantage of the affine parameter dependence of <span>$A(p)$</span> and <span>$b(p)$</span>, and uses an upper-bound error approximator to produce a reduced basis that approximates solutions with comparable error compared to the strong greedy algorithm and the POD algorithm without needing to compute all solutions ahead of time.</p><h3 id="References:"><a class="docs-heading-anchor" href="#References:">References:</a><a id="References:-1"></a><a class="docs-heading-anchor-permalink" href="#References:" title="Permalink"></a></h3><ol><li>D.B.P. Huynh, G. Rozza, S. Sen, A.T. Patera. A successive constraint linear optimization method for lower bounds of parametric coercivity and inf–sup stability constants. Comptes Rendus Mathematique. Volume 345, Issue 8. 2007. Pages 473-478. https://doi.org/10.1016/j.crma.2007.09.019.</li><li>Quarteroni, Alfio, Andrea Manzoni, and Federico Negri. Reduced Basis Methods for Partial Differential Equations. Vol. 92. UNITEXT. Cham: Springer International Publishing, 2016. http://link.springer.com/10.1007/978-3-319-15431-2.</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="index.html">« Models and Reductors</a><a class="docs-footer-nextpage" href="docs.html">Additional Docstrings »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 8 August 2024 15:13">Thursday 8 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
